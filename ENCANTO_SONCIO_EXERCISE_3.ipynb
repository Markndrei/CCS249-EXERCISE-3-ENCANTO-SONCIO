{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Creating Bigram and Trigram Language Model with Perplexity Calculation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is made by: **Mark Andrei Encanto** & **Ethan Gabriel Soncio**\n",
    "\n",
    "For this notebook, we will have to follow three steps:\n",
    "1. Use `Wikipedia Module` to access *any* topic with a **limit of 1000 words**.\n",
    "2. Traing two models: the `Bigram Language Model` & the `Trigram Language Model`.\n",
    "3. Using a *test sentence*, create a function to determine the **perplexity score** of each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Wikipedia Python Module to Access a Corpus\n",
    "\n",
    "In this exercise, we will utilize the `wikipedia` Python module to fetch content from Wikipedia. This content will serve as our corpus for building language models. The steps involved are as follows:\n",
    "\n",
    "1. **Accessing Wikipedia Content**: We will use the `wikipedia` module to fetch a random Wikipedia article. This module provides a simple interface to access and parse Wikipedia content.\n",
    "\n",
    "2. **Limiting the Corpus**: To ensure our corpus is manageable, we will limit the content to 1000 words. This will help in efficiently training our language models without overwhelming computational resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus fetched successfully!\n",
      "Corpus length: 1000 words\n",
      "\n",
      "Corpus:  In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a model inspired by the structure and function of biological neural networks in animal brains. A neural network consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. Artificial neuron models that mimic biological neurons more closely have also been recently investigated and shown to significantly improve performance. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least two hidden layers. Artificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information. == Training == Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network's parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset. Gradient-based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data. == History == === Early work === Today's deep neural networks are based on early work in statistics over 200 years ago. The simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes with linear activation functions; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. Historically, digital computers such as the von Neumann model operate via the execution of explicit instructions with access to memory by a number of processors. Some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing. Warren McCulloch and Walter Pitts (1943) considered a non-learning computational model for neural networks. This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence. In the late 1940s, D. O. Hebb proposed a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. It was used in many early neural networks, such as Rosenblatt's perceptron and the Hopfield network. Farley and Clark (1954) used computational machines to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956). In 1958, psychologist Frank Rosenblatt described the perceptron, one of the first implemented artificial neural networks, funded by the United States Office of Naval Research. R. D. Joseph (1960) mentions an even earlier perceptron-like device by Farley and Clark: \"Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device.\" However, \"they dropped the subject.\" The perceptron raised public excitement for research in Artificial Neural Networks, causing the US government to drastically increase funding. This contributed to \"the Golden Age of AI\" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. The first perceptrons did not have adaptive hidden units. However, Joseph (1960) also discussed multilayer perceptrons with an adaptive hidden layer. Rosenblatt (1962): section 16 cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight. Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning. === Deep learning breakthroughs in the 1960s and 1970s === Fundamental research was conducted on ANNs in the 1960s and 1970s. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in the Soviet Union (1965). They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates.\" The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "# Set the topic and fetch the content\n",
    "topic = \"Neural Networks\"\n",
    "wikipedia.set_lang(\"en\")  # Set language to English\n",
    "page = wikipedia.page(topic)\n",
    "corpus = \" \".join(page.content.split()[:1000])  # Limit to 1000 words\n",
    "\n",
    "print(\"Corpus fetched successfully!\")\n",
    "print(f\"Corpus length: {len(corpus.split())} words\")  # Verify word count\n",
    "print(\"\\nCorpus: \", corpus)  # Print the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Bigram and Trigram Language Models\n",
    "\n",
    "In this section, we will train two language models: a **Bigram Language Model** and a **Trigram Language Model**. These models help estimate the probability of a word given its preceding word(s).\n",
    "\n",
    "The steps involved are as follows:\n",
    "1. **Corpus Preprocessing**: We will *tokenize* the corpus and normalize it through `lowercasing` the tokens and `removing punctuation` in the group of words.\n",
    "2. **Probability Calculations**: We will calculate the `Bigram` and `Trigram` probabiities\n",
    "\n",
    "#### **Bigram Language Model**\n",
    "A **Bigram Language Model** calculates the probability of a word based on the previous word. The formula for the bigram probability is:\n",
    "\n",
    "$$\n",
    "P(w_i \\mid w_{i-1}) = \\frac{C(w_{i-1}, w_i)}{C(w_{i-1})}\n",
    "$$\n",
    "\n",
    "`Where:`\n",
    "> $C(w_{i-1}, w_i)$ is the count of the bigram (previous word, current word).\\\n",
    "> $C(w_{i-1})$ is the count of the previous word.\n",
    "\n",
    "#### **Bigram Language Model**\n",
    "A **Trigram Language Model** calculates the probability of a word based on the two preceding words. The formula for the trigram probability is:\n",
    "\n",
    "$$\n",
    "P(w_i \\mid w_{i-2}, w_{i-1}) = \\frac{C(w_{i-2}, w_{i-1}, w_i)}{C(w_{i-2}, w_{i-1})}\n",
    "$$\n",
    "\n",
    "`Where:`\n",
    "> $C(w_{i-2}, w_{i-1}, w_i)$ is the count of the trigram (two previous words, current word).\n",
    "> $C(w_{i-2}, w_{i-1})$ is the count of the two previous words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 978\n",
      "Number of uni-grams: 411\n",
      "Number of bi-grams: 847\n",
      "Number of tri-grams: 955\n",
      "Vocabulary size: 411\n",
      "Tokens ['in', 'machine', 'learning', 'a', 'neural', 'network', 'also', 'artificial', 'neural', 'network', 'or', 'neural', 'net', 'abbreviated', 'ann', 'or', 'nn', 'is', 'a', 'model', 'inspired', 'by', 'the', 'structure', 'and', 'function', 'of', 'biological', 'neural', 'networks', 'in', 'animal', 'brains', 'a', 'neural', 'network', 'consists', 'of', 'connected', 'units', 'or', 'nodes', 'called', 'artificial', 'neurons', 'which', 'loosely', 'model', 'the', 'neurons', 'in', 'the', 'brain', 'artificial', 'neuron', 'models', 'that', 'mimic', 'biological', 'neurons', 'more', 'closely', 'have', 'also', 'been', 'recently', 'investigated', 'and', 'shown', 'to', 'significantly', 'improve', 'performance', 'these', 'are', 'connected', 'by', 'edges', 'which', 'model', 'the', 'synapses', 'in', 'the', 'brain', 'each', 'artificial', 'neuron', 'receives', 'signals', 'from', 'connected', 'neurons', 'then', 'processes', 'them', 'and', 'sends', 'a', 'signal', 'to', 'other', 'connected', 'neurons', 'the', 'signal', 'is', 'a', 'real', 'number', 'and', 'the', 'output', 'of', 'each', 'neuron', 'is', 'computed', 'by', 'some', 'nonlinear', 'function', 'of', 'the', 'sum', 'of', 'its', 'inputs', 'called', 'the', 'activation', 'function', 'the', 'strength', 'of', 'the', 'signal', 'at', 'each', 'connection', 'is', 'determined', 'by', 'a', 'weight', 'which', 'adjusts', 'during', 'the', 'learning', 'process', 'typically', 'neurons', 'are', 'aggregated', 'into', 'layers', 'different', 'layers', 'may', 'perform', 'different', 'transformations', 'on', 'their', 'inputs', 'signals', 'travel', 'from', 'the', 'first', 'layer', 'the', 'input', 'layer', 'to', 'the', 'last', 'layer', 'the', 'output', 'layer', 'possibly', 'passing', 'through', 'multiple', 'intermediate', 'layers', 'hidden', 'layers', 'a', 'network', 'is', 'typically', 'called', 'a', 'deep', 'neural', 'network', 'if', 'it', 'has', 'at', 'least', 'two', 'hidden', 'layers', 'artificial', 'neural', 'networks', 'are', 'used', 'for', 'various', 'tasks', 'including', 'predictive', 'modeling', 'adaptive', 'control', 'and', 'solving', 'problems', 'in', 'artificial', 'intelligence', 'they', 'can', 'learn', 'from', 'experience', 'and', 'can', 'derive', 'conclusions', 'from', 'a', 'complex', 'and', 'seemingly', 'unrelated', 'set', 'of', 'information', 'training', 'neural', 'networks', 'are', 'typically', 'trained', 'through', 'empirical', 'risk', 'minimization', 'this', 'method', 'is', 'based', 'on', 'the', 'idea', 'of', 'optimizing', 'the', 'networks', 'parameters', 'to', 'minimize', 'the', 'difference', 'or', 'empirical', 'risk', 'between', 'the', 'predicted', 'output', 'and', 'the', 'actual', 'target', 'values', 'in', 'a', 'given', 'dataset', 'gradientbased', 'methods', 'such', 'as', 'backpropagation', 'are', 'usually', 'used', 'to', 'estimate', 'the', 'parameters', 'of', 'the', 'network', 'during', 'the', 'training', 'phase', 'anns', 'learn', 'from', 'labeled', 'training', 'data', 'by', 'iteratively', 'updating', 'their', 'parameters', 'to', 'minimize', 'a', 'defined', 'loss', 'function', 'this', 'method', 'allows', 'the', 'network', 'to', 'generalize', 'to', 'unseen', 'data', 'history', 'early', 'work', 'todays', 'deep', 'neural', 'networks', 'are', 'based', 'on', 'early', 'work', 'in', 'statistics', 'over', 'years', 'ago', 'the', 'simplest', 'kind', 'of', 'feedforward', 'neural', 'network', 'fnn', 'is', 'a', 'linear', 'network', 'which', 'consists', 'of', 'a', 'single', 'layer', 'of', 'output', 'nodes', 'with', 'linear', 'activation', 'functions', 'the', 'inputs', 'are', 'fed', 'directly', 'to', 'the', 'outputs', 'via', 'a', 'series', 'of', 'weights', 'the', 'sum', 'of', 'the', 'products', 'of', 'the', 'weights', 'and', 'the', 'inputs', 'is', 'calculated', 'at', 'each', 'node', 'the', 'mean', 'squared', 'errors', 'between', 'these', 'calculated', 'outputs', 'and', 'the', 'given', 'target', 'values', 'are', 'minimized', 'by', 'creating', 'an', 'adjustment', 'to', 'the', 'weights', 'this', 'technique', 'has', 'been', 'known', 'for', 'over', 'two', 'centuries', 'as', 'the', 'method', 'of', 'least', 'squares', 'or', 'linear', 'regression', 'it', 'was', 'used', 'as', 'a', 'means', 'of', 'finding', 'a', 'good', 'rough', 'linear', 'fit', 'to', 'a', 'set', 'of', 'points', 'by', 'legendre', 'and', 'gauss', 'for', 'the', 'prediction', 'of', 'planetary', 'movement', 'historically', 'digital', 'computers', 'such', 'as', 'the', 'von', 'neumann', 'model', 'operate', 'via', 'the', 'execution', 'of', 'explicit', 'instructions', 'with', 'access', 'to', 'memory', 'by', 'a', 'number', 'of', 'processors', 'some', 'neural', 'networks', 'on', 'the', 'other', 'hand', 'originated', 'from', 'efforts', 'to', 'model', 'information', 'processing', 'in', 'biological', 'systems', 'through', 'the', 'framework', 'of', 'connectionism', 'unlike', 'the', 'von', 'neumann', 'model', 'connectionist', 'computing', 'does', 'not', 'separate', 'memory', 'and', 'processing', 'warren', 'mcculloch', 'and', 'walter', 'pitts', 'considered', 'a', 'nonlearning', 'computational', 'model', 'for', 'neural', 'networks', 'this', 'model', 'paved', 'the', 'way', 'for', 'research', 'to', 'split', 'into', 'two', 'approaches', 'one', 'approach', 'focused', 'on', 'biological', 'processes', 'while', 'the', 'other', 'focused', 'on', 'the', 'application', 'of', 'neural', 'networks', 'to', 'artificial', 'intelligence', 'in', 'the', 'late', 's', 'd', 'o', 'hebb', 'proposed', 'a', 'learning', 'hypothesis', 'based', 'on', 'the', 'mechanism', 'of', 'neural', 'plasticity', 'that', 'became', 'known', 'as', 'hebbian', 'learning', 'it', 'was', 'used', 'in', 'many', 'early', 'neural', 'networks', 'such', 'as', 'rosenblatts', 'perceptron', 'and', 'the', 'hopfield', 'network', 'farley', 'and', 'clark', 'used', 'computational', 'machines', 'to', 'simulate', 'a', 'hebbian', 'network', 'other', 'neural', 'network', 'computational', 'machines', 'were', 'created', 'by', 'rochester', 'holland', 'habit', 'and', 'duda', 'in', 'psychologist', 'frank', 'rosenblatt', 'described', 'the', 'perceptron', 'one', 'of', 'the', 'first', 'implemented', 'artificial', 'neural', 'networks', 'funded', 'by', 'the', 'united', 'states', 'office', 'of', 'naval', 'research', 'r', 'd', 'joseph', 'mentions', 'an', 'even', 'earlier', 'perceptronlike', 'device', 'by', 'farley', 'and', 'clark', 'farley', 'and', 'clark', 'of', 'mit', 'lincoln', 'laboratory', 'actually', 'preceded', 'rosenblatt', 'in', 'the', 'development', 'of', 'a', 'perceptronlike', 'device', 'however', 'they', 'dropped', 'the', 'subject', 'the', 'perceptron', 'raised', 'public', 'excitement', 'for', 'research', 'in', 'artificial', 'neural', 'networks', 'causing', 'the', 'us', 'government', 'to', 'drastically', 'increase', 'funding', 'this', 'contributed', 'to', 'the', 'golden', 'age', 'of', 'ai', 'fueled', 'by', 'the', 'optimistic', 'claims', 'made', 'by', 'computer', 'scientists', 'regarding', 'the', 'ability', 'of', 'perceptrons', 'to', 'emulate', 'human', 'intelligence', 'the', 'first', 'perceptrons', 'did', 'not', 'have', 'adaptive', 'hidden', 'units', 'however', 'joseph', 'also', 'discussed', 'multilayer', 'perceptrons', 'with', 'an', 'adaptive', 'hidden', 'layer', 'rosenblatt', 'section', 'cited', 'and', 'adopted', 'these', 'ideas', 'also', 'crediting', 'work', 'by', 'h', 'd', 'block', 'and', 'b', 'w', 'knight', 'unfortunately', 'these', 'early', 'efforts', 'did', 'not', 'lead', 'to', 'a', 'working', 'learning', 'algorithm', 'for', 'hidden', 'units', 'ie', 'deep', 'learning', 'deep', 'learning', 'breakthroughs', 'in', 'the', 's', 'and', 's', 'fundamental', 'research', 'was', 'conducted', 'on', 'anns', 'in', 'the', 's', 'and', 's', 'the', 'first', 'working', 'deep', 'learning', 'algorithm', 'was', 'the', 'group', 'method', 'of', 'data', 'handling', 'a', 'method', 'to', 'train', 'arbitrarily', 'deep', 'neural', 'networks', 'published', 'by', 'alexey', 'ivakhnenko', 'and', 'lapa', 'in', 'the', 'soviet', 'union', 'they', 'regarded', 'it', 'as', 'a', 'form', 'of', 'polynomial', 'regression', 'or', 'a', 'generalization', 'of', 'rosenblatts', 'perceptron', 'a', 'paper', 'described', 'a', 'deep', 'network', 'with', 'eight', 'layers', 'trained', 'by', 'this', 'method', 'which', 'is', 'based', 'on', 'layer', 'by', 'layer', 'training', 'through', 'regression', 'analysis', 'superfluous', 'hidden', 'units', 'are', 'pruned', 'using', 'a', 'separate', 'validation', 'set', 'since', 'the', 'activation', 'functions', 'of', 'the', 'nodes', 'are', 'kolmogorovgabor', 'polynomials', 'these', 'were', 'also', 'the', 'first', 'deep', 'networks', 'with', 'multiplicative', 'units', 'or', 'gates', 'the', 'first', 'deep', 'learning', 'multilayer', 'perceptron', 'trained', 'by', 'stochastic', 'gradient', 'descent', 'was', 'published', 'in', 'by', 'shunichi', 'amari', 'in', 'computer', 'experiments', 'conducted', 'by', 'amaris', 'student', 'saito', 'a', 'five', 'layer', 'mlp', 'with', 'two', 'modifiable', 'layers', 'learned', 'internal', 'representations', 'to', 'classify', 'nonlinearily', 'separable', 'pattern', 'classes', 'subsequent', 'developments', 'in', 'hardware', 'and', 'hyperparameter', 'tunings', 'have', 'made', 'endtoend', 'stochastic', 'gradient', 'descent', 'the', 'currently', 'dominant']\n"
     ]
    }
   ],
   "source": [
    "# Corpus Preprocessing\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure corpus is a string\n",
    "if isinstance(corpus, list):\n",
    "\tcorpus = \" \".join(corpus)\n",
    "\n",
    "# Tokenize the corpus\n",
    "corpus = corpus.lower()  # Convert to lowercase\n",
    "corpus = re.sub(r'[^a-zA-Z\\s]', '', corpus)  # Remove punctuation\n",
    "tokens = word_tokenize(corpus)\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "\n",
    "# Extracting uni-grams\n",
    "uni_grams = Counter(tokens)\n",
    "print(f\"Number of uni-grams: {len(uni_grams)}\")\n",
    "\n",
    "# Extracting bi-grams\n",
    "bi_grams = Counter(bigrams(tokens))\n",
    "print(f\"Number of bi-grams: {len(bi_grams)}\")\n",
    "\n",
    "# Extracting tri-grams\n",
    "tri_grams = Counter(trigrams(tokens))\n",
    "print(f\"Number of tri-grams: {len(tri_grams)}\")\n",
    "\n",
    "# Initialize uni_grams as vocabulary\n",
    "vocab = len(set(tokens))\n",
    "print(f\"Vocabulary size: {vocab}\")\n",
    "\n",
    "# Print the tokens\n",
    "print(\"Tokens\",tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Probabilities:\n",
      "                      Bigram**  **Probability\n",
      "                 (in, machine)       0.004651\n",
      "           (machine, learning)       0.004854\n",
      "                 (learning, a)       0.004762\n",
      "                   (a, neural)       0.006803\n",
      "             (neural, network)       0.016279\n",
      "               (network, also)       0.004717\n",
      "            (also, artificial)       0.004808\n",
      "          (artificial, neural)       0.011905\n",
      "                 (network, or)       0.004717\n",
      "                  (or, neural)       0.004785\n",
      "                 (neural, net)       0.004651\n",
      "            (net, abbreviated)       0.004854\n",
      "            (abbreviated, ann)       0.004854\n",
      "                     (ann, or)       0.004854\n",
      "                      (or, nn)       0.004785\n",
      "                      (nn, is)       0.004854\n",
      "                       (is, a)       0.009524\n",
      "                    (a, model)       0.004535\n",
      "             (model, inspired)       0.004773\n",
      "                (inspired, by)       0.004854\n",
      "                     (by, the)       0.009281\n",
      "              (the, structure)       0.004167\n",
      "              (structure, and)       0.004854\n",
      "               (and, function)       0.004598\n",
      "                (function, of)       0.007229\n",
      "              (of, biological)       0.004494\n",
      "          (biological, neural)       0.004819\n",
      "            (neural, networks)       0.027907\n",
      "                (networks, in)       0.004717\n",
      "                  (in, animal)       0.004651\n",
      "              (animal, brains)       0.004854\n",
      "                   (brains, a)       0.004854\n",
      "           (network, consists)       0.004717\n",
      "                (consists, of)       0.007264\n",
      "               (of, connected)       0.004494\n",
      "            (connected, units)       0.004819\n",
      "                   (units, or)       0.007212\n",
      "                   (or, nodes)       0.004785\n",
      "               (nodes, called)       0.004831\n",
      "          (called, artificial)       0.004831\n",
      "         (artificial, neurons)       0.004762\n",
      "              (neurons, which)       0.004796\n",
      "              (which, loosely)       0.004808\n",
      "              (loosely, model)       0.004854\n",
      "                  (model, the)       0.007160\n",
      "                (the, neurons)       0.004167\n",
      "                 (neurons, in)       0.004796\n",
      "                     (in, the)       0.018605\n",
      "                  (the, brain)       0.006250\n",
      "           (brain, artificial)       0.004843\n",
      "          (artificial, neuron)       0.007143\n",
      "              (neuron, models)       0.004831\n",
      "                (models, that)       0.004854\n",
      "                 (that, mimic)       0.004843\n",
      "           (mimic, biological)       0.004854\n",
      "         (biological, neurons)       0.004819\n",
      "               (neurons, more)       0.004796\n",
      "               (more, closely)       0.004854\n",
      "               (closely, have)       0.004854\n",
      "                  (have, also)       0.004831\n",
      "                  (also, been)       0.004808\n",
      "              (been, recently)       0.004843\n",
      "      (recently, investigated)       0.004854\n",
      "           (investigated, and)       0.004854\n",
      "                  (and, shown)       0.004598\n",
      "                   (shown, to)       0.004854\n",
      "           (to, significantly)       0.004619\n",
      "      (significantly, improve)       0.004854\n",
      "        (improve, performance)       0.004854\n",
      "          (performance, these)       0.004854\n",
      "                  (these, are)       0.004808\n",
      "              (are, connected)       0.004751\n",
      "               (connected, by)       0.004819\n",
      "                   (by, edges)       0.004640\n",
      "                (edges, which)       0.004854\n",
      "                (which, model)       0.004808\n",
      "               (the, synapses)       0.004167\n",
      "                (synapses, in)       0.004854\n",
      "                 (brain, each)       0.004843\n",
      "            (each, artificial)       0.004819\n",
      "            (neuron, receives)       0.004831\n",
      "           (receives, signals)       0.004854\n",
      "               (signals, from)       0.004843\n",
      "             (from, connected)       0.004796\n",
      "          (connected, neurons)       0.007229\n",
      "               (neurons, then)       0.004796\n",
      "             (then, processes)       0.004854\n",
      "             (processes, them)       0.004843\n",
      "                   (them, and)       0.004854\n",
      "                  (and, sends)       0.004598\n",
      "                    (sends, a)       0.004854\n",
      "                   (a, signal)       0.004535\n",
      "                  (signal, to)       0.004831\n",
      "                   (to, other)       0.004619\n",
      "            (other, connected)       0.004819\n",
      "                (neurons, the)       0.004796\n",
      "                 (the, signal)       0.006250\n",
      "                  (signal, is)       0.004831\n",
      "                     (a, real)       0.004535\n",
      "                (real, number)       0.004854\n",
      "                 (number, and)       0.004843\n",
      "                    (and, the)       0.013793\n",
      "                 (the, output)       0.006250\n",
      "                  (output, of)       0.004819\n",
      "                    (of, each)       0.004494\n",
      "                (each, neuron)       0.004819\n",
      "                  (neuron, is)       0.004831\n",
      "                (is, computed)       0.004762\n",
      "                (computed, by)       0.004854\n",
      "                    (by, some)       0.004640\n",
      "             (some, nonlinear)       0.004843\n",
      "         (nonlinear, function)       0.004854\n",
      "                     (of, the)       0.017978\n",
      "                    (the, sum)       0.006250\n",
      "                     (sum, of)       0.007264\n",
      "                     (of, its)       0.004494\n",
      "                 (its, inputs)       0.004854\n",
      "              (inputs, called)       0.004819\n",
      "                 (called, the)       0.004831\n",
      "             (the, activation)       0.006250\n",
      "        (activation, function)       0.004831\n",
      "               (function, the)       0.004819\n",
      "               (the, strength)       0.004167\n",
      "                (strength, of)       0.004854\n",
      "                  (signal, at)       0.004831\n",
      "                    (at, each)       0.007246\n",
      "            (each, connection)       0.004819\n",
      "              (connection, is)       0.004854\n",
      "              (is, determined)       0.004762\n",
      "              (determined, by)       0.004854\n",
      "                       (by, a)       0.006961\n",
      "                   (a, weight)       0.004535\n",
      "               (weight, which)       0.004854\n",
      "              (which, adjusts)       0.004808\n",
      "             (adjusts, during)       0.004854\n",
      "                 (during, the)       0.007264\n",
      "               (the, learning)       0.004167\n",
      "           (learning, process)       0.004762\n",
      "          (process, typically)       0.004854\n",
      "          (typically, neurons)       0.004831\n",
      "                (neurons, are)       0.004796\n",
      "             (are, aggregated)       0.004751\n",
      "            (aggregated, into)       0.004854\n",
      "                (into, layers)       0.004843\n",
      "           (layers, different)       0.004785\n",
      "           (different, layers)       0.004843\n",
      "                 (layers, may)       0.004785\n",
      "                (may, perform)       0.004854\n",
      "          (perform, different)       0.004854\n",
      "  (different, transformations)       0.004843\n",
      "         (transformations, on)       0.004854\n",
      "                   (on, their)       0.004762\n",
      "               (their, inputs)       0.004843\n",
      "             (inputs, signals)       0.004819\n",
      "             (signals, travel)       0.004843\n",
      "                (travel, from)       0.004854\n",
      "                   (from, the)       0.004796\n",
      "                  (the, first)       0.014583\n",
      "                (first, layer)       0.004796\n",
      "                  (layer, the)       0.007143\n",
      "                  (the, input)       0.004167\n",
      "                (input, layer)       0.004854\n",
      "                   (layer, to)       0.004762\n",
      "                     (to, the)       0.011547\n",
      "                   (the, last)       0.004167\n",
      "                 (last, layer)       0.004854\n",
      "               (output, layer)       0.004819\n",
      "             (layer, possibly)       0.004762\n",
      "           (possibly, passing)       0.004854\n",
      "            (passing, through)       0.004854\n",
      "           (through, multiple)       0.004819\n",
      "      (multiple, intermediate)       0.004854\n",
      "        (intermediate, layers)       0.004854\n",
      "              (layers, hidden)       0.004785\n",
      "              (hidden, layers)       0.007194\n",
      "                   (layers, a)       0.004785\n",
      "                  (a, network)       0.004535\n",
      "                 (network, is)       0.004717\n",
      "               (is, typically)       0.004762\n",
      "           (typically, called)       0.004831\n",
      "                   (called, a)       0.004831\n",
      "                     (a, deep)       0.006803\n",
      "                (deep, neural)       0.009524\n",
      "                 (network, if)       0.004717\n",
      "                      (if, it)       0.004854\n",
      "                     (it, has)       0.004819\n",
      "                     (has, at)       0.004843\n",
      "                   (at, least)       0.004831\n",
      "                  (least, two)       0.004843\n",
      "                 (two, hidden)       0.004819\n",
      "          (layers, artificial)       0.004785\n",
      "               (networks, are)       0.009434\n",
      "                   (are, used)       0.004751\n",
      "                   (used, for)       0.004808\n",
      "                (for, various)       0.004785\n",
      "              (various, tasks)       0.004854\n",
      "            (tasks, including)       0.004854\n",
      "       (including, predictive)       0.004854\n",
      "        (predictive, modeling)       0.004854\n",
      "          (modeling, adaptive)       0.004854\n",
      "           (adaptive, control)       0.004831\n",
      "                (control, and)       0.004854\n",
      "                (and, solving)       0.004598\n",
      "           (solving, problems)       0.004854\n",
      "                (problems, in)       0.004854\n",
      "              (in, artificial)       0.006977\n",
      "    (artificial, intelligence)       0.007143\n",
      "          (intelligence, they)       0.004831\n",
      "                   (they, can)       0.004831\n",
      "                  (can, learn)       0.004843\n",
      "                 (learn, from)       0.007264\n",
      "            (from, experience)       0.004796\n",
      "             (experience, and)       0.004854\n",
      "                    (and, can)       0.004598\n",
      "                 (can, derive)       0.004843\n",
      "         (derive, conclusions)       0.004854\n",
      "           (conclusions, from)       0.004854\n",
      "                     (from, a)       0.004796\n",
      "                  (a, complex)       0.004535\n",
      "                (complex, and)       0.004854\n",
      "              (and, seemingly)       0.004598\n",
      "        (seemingly, unrelated)       0.004854\n",
      "              (unrelated, set)       0.004854\n",
      "                     (set, of)       0.007246\n",
      "             (of, information)       0.004494\n",
      "       (information, training)       0.004843\n",
      "            (training, neural)       0.004819\n",
      "              (are, typically)       0.004751\n",
      "          (typically, trained)       0.004831\n",
      "            (trained, through)       0.004831\n",
      "          (through, empirical)       0.004819\n",
      "             (empirical, risk)       0.007264\n",
      "          (risk, minimization)       0.004843\n",
      "          (minimization, this)       0.004854\n",
      "                (this, method)       0.009592\n",
      "                  (method, is)       0.004796\n",
      "                   (is, based)       0.007143\n",
      "                   (based, on)       0.012048\n",
      "                     (on, the)       0.011905\n",
      "                   (the, idea)       0.004167\n",
      "                    (idea, of)       0.004854\n",
      "              (of, optimizing)       0.004494\n",
      "             (optimizing, the)       0.004854\n",
      "               (the, networks)       0.004167\n",
      "        (networks, parameters)       0.004717\n",
      "              (parameters, to)       0.007246\n",
      "                (to, minimize)       0.006928\n",
      "               (minimize, the)       0.004843\n",
      "             (the, difference)       0.004167\n",
      "              (difference, or)       0.004854\n",
      "               (or, empirical)       0.004785\n",
      "               (risk, between)       0.004843\n",
      "                (between, the)       0.004843\n",
      "              (the, predicted)       0.004167\n",
      "           (predicted, output)       0.004854\n",
      "                 (output, and)       0.004819\n",
      "                 (the, actual)       0.004167\n",
      "              (actual, target)       0.004854\n",
      "              (target, values)       0.007264\n",
      "                  (values, in)       0.004843\n",
      "                       (in, a)       0.004651\n",
      "                    (a, given)       0.004535\n",
      "              (given, dataset)       0.004843\n",
      "      (dataset, gradientbased)       0.004854\n",
      "      (gradientbased, methods)       0.004854\n",
      "               (methods, such)       0.004854\n",
      "                    (such, as)       0.009662\n",
      "         (as, backpropagation)       0.004785\n",
      "        (backpropagation, are)       0.004854\n",
      "                (are, usually)       0.004751\n",
      "               (usually, used)       0.004854\n",
      "                    (used, to)       0.004808\n",
      "                (to, estimate)       0.004619\n",
      "               (estimate, the)       0.004854\n",
      "             (the, parameters)       0.004167\n",
      "              (parameters, of)       0.004831\n",
      "                (the, network)       0.006250\n",
      "             (network, during)       0.004717\n",
      "               (the, training)       0.004167\n",
      "             (training, phase)       0.004819\n",
      "                 (phase, anns)       0.004854\n",
      "                 (anns, learn)       0.004843\n",
      "               (from, labeled)       0.004796\n",
      "           (labeled, training)       0.004854\n",
      "              (training, data)       0.004819\n",
      "                    (data, by)       0.004831\n",
      "             (by, iteratively)       0.004640\n",
      "       (iteratively, updating)       0.004854\n",
      "             (updating, their)       0.004854\n",
      "           (their, parameters)       0.004843\n",
      "                 (minimize, a)       0.004843\n",
      "                  (a, defined)       0.004535\n",
      "               (defined, loss)       0.004854\n",
      "              (loss, function)       0.004854\n",
      "              (function, this)       0.004819\n",
      "              (method, allows)       0.004796\n",
      "                 (allows, the)       0.004854\n",
      "                 (network, to)       0.004717\n",
      "              (to, generalize)       0.004619\n",
      "              (generalize, to)       0.004854\n",
      "                  (to, unseen)       0.004619\n",
      "                (unseen, data)       0.004854\n",
      "               (data, history)       0.004831\n",
      "              (history, early)       0.004854\n",
      "                 (early, work)       0.007229\n",
      "                (work, todays)       0.004831\n",
      "                (todays, deep)       0.004854\n",
      "                  (are, based)       0.004751\n",
      "                   (on, early)       0.004762\n",
      "                    (work, in)       0.004831\n",
      "              (in, statistics)       0.004651\n",
      "            (statistics, over)       0.004854\n",
      "                 (over, years)       0.004843\n",
      "                  (years, ago)       0.004854\n",
      "                    (ago, the)       0.004854\n",
      "               (the, simplest)       0.004167\n",
      "              (simplest, kind)       0.004854\n",
      "                    (kind, of)       0.004854\n",
      "             (of, feedforward)       0.004494\n",
      "         (feedforward, neural)       0.004854\n",
      "                (network, fnn)       0.004717\n",
      "                     (fnn, is)       0.004854\n",
      "                   (a, linear)       0.004535\n",
      "             (linear, network)       0.004819\n",
      "              (network, which)       0.004717\n",
      "             (which, consists)       0.004808\n",
      "                       (of, a)       0.006742\n",
      "                   (a, single)       0.004535\n",
      "               (single, layer)       0.004854\n",
      "                   (layer, of)       0.004762\n",
      "                  (of, output)       0.004494\n",
      "               (output, nodes)       0.004819\n",
      "                 (nodes, with)       0.004831\n",
      "                (with, linear)       0.004796\n",
      "          (linear, activation)       0.004819\n",
      "       (activation, functions)       0.007246\n",
      "              (functions, the)       0.004843\n",
      "                 (the, inputs)       0.006250\n",
      "                 (inputs, are)       0.004819\n",
      "                    (are, fed)       0.004751\n",
      "               (fed, directly)       0.004854\n",
      "                (directly, to)       0.004854\n",
      "                (the, outputs)       0.004167\n",
      "                (outputs, via)       0.004843\n",
      "                      (via, a)       0.004843\n",
      "                   (a, series)       0.004535\n",
      "                  (series, of)       0.004854\n",
      "                 (of, weights)       0.004494\n",
      "                (weights, the)       0.004831\n",
      "               (the, products)       0.004167\n",
      "                (products, of)       0.004854\n",
      "                (the, weights)       0.006250\n",
      "                (weights, and)       0.004831\n",
      "                  (inputs, is)       0.004819\n",
      "              (is, calculated)       0.004762\n",
      "              (calculated, at)       0.004843\n",
      "                  (each, node)       0.004819\n",
      "                   (node, the)       0.004854\n",
      "                   (the, mean)       0.004167\n",
      "               (mean, squared)       0.004854\n",
      "             (squared, errors)       0.004854\n",
      "             (errors, between)       0.004854\n",
      "              (between, these)       0.004843\n",
      "           (these, calculated)       0.004808\n",
      "         (calculated, outputs)       0.004843\n",
      "                (outputs, and)       0.004843\n",
      "                  (the, given)       0.004167\n",
      "               (given, target)       0.004843\n",
      "                 (values, are)       0.004843\n",
      "              (are, minimized)       0.004751\n",
      "               (minimized, by)       0.004854\n",
      "                (by, creating)       0.004640\n",
      "                (creating, an)       0.004854\n",
      "              (an, adjustment)       0.004831\n",
      "              (adjustment, to)       0.004854\n",
      "               (weights, this)       0.004831\n",
      "             (this, technique)       0.004796\n",
      "              (technique, has)       0.004854\n",
      "                   (has, been)       0.004843\n",
      "                 (been, known)       0.004843\n",
      "                  (known, for)       0.004843\n",
      "                   (for, over)       0.004785\n",
      "                   (over, two)       0.004843\n",
      "              (two, centuries)       0.004819\n",
      "               (centuries, as)       0.004854\n",
      "                     (as, the)       0.007177\n",
      "                 (the, method)       0.004167\n",
      "                  (method, of)       0.007194\n",
      "                   (of, least)       0.004494\n",
      "              (least, squares)       0.004843\n",
      "                 (squares, or)       0.004854\n",
      "                  (or, linear)       0.004785\n",
      "          (linear, regression)       0.004819\n",
      "              (regression, it)       0.004831\n",
      "                     (it, was)       0.007229\n",
      "                   (was, used)       0.007212\n",
      "                    (used, as)       0.004808\n",
      "                       (as, a)       0.007177\n",
      "                    (a, means)       0.004535\n",
      "                   (means, of)       0.004854\n",
      "                 (of, finding)       0.004494\n",
      "                  (finding, a)       0.004854\n",
      "                     (a, good)       0.004535\n",
      "                 (good, rough)       0.004854\n",
      "               (rough, linear)       0.004854\n",
      "                 (linear, fit)       0.004819\n",
      "                     (fit, to)       0.004854\n",
      "                       (to, a)       0.006928\n",
      "                      (a, set)       0.004535\n",
      "                  (of, points)       0.004494\n",
      "                  (points, by)       0.004854\n",
      "                (by, legendre)       0.004640\n",
      "               (legendre, and)       0.004854\n",
      "                  (and, gauss)       0.004598\n",
      "                  (gauss, for)       0.004854\n",
      "                    (for, the)       0.004785\n",
      "             (the, prediction)       0.004167\n",
      "              (prediction, of)       0.004854\n",
      "               (of, planetary)       0.004494\n",
      "         (planetary, movement)       0.004854\n",
      "      (movement, historically)       0.004854\n",
      "       (historically, digital)       0.004854\n",
      "          (digital, computers)       0.004854\n",
      "             (computers, such)       0.004854\n",
      "                    (the, von)       0.006250\n",
      "                (von, neumann)       0.007264\n",
      "              (neumann, model)       0.007264\n",
      "              (model, operate)       0.004773\n",
      "                (operate, via)       0.004854\n",
      "                    (via, the)       0.004843\n",
      "              (the, execution)       0.004167\n",
      "               (execution, of)       0.004854\n",
      "                (of, explicit)       0.004494\n",
      "      (explicit, instructions)       0.004854\n",
      "          (instructions, with)       0.004854\n",
      "                (with, access)       0.004796\n",
      "                  (access, to)       0.004854\n",
      "                  (to, memory)       0.004619\n",
      "                  (memory, by)       0.004843\n",
      "                   (a, number)       0.004535\n",
      "                  (number, of)       0.004843\n",
      "              (of, processors)       0.004494\n",
      "            (processors, some)       0.004854\n",
      "                (some, neural)       0.004843\n",
      "                (networks, on)       0.004717\n",
      "                  (the, other)       0.006250\n",
      "                 (other, hand)       0.004819\n",
      "            (hand, originated)       0.004854\n",
      "            (originated, from)       0.004854\n",
      "               (from, efforts)       0.004796\n",
      "                 (efforts, to)       0.004843\n",
      "                   (to, model)       0.004619\n",
      "          (model, information)       0.004773\n",
      "     (information, processing)       0.004843\n",
      "              (processing, in)       0.004843\n",
      "              (in, biological)       0.004651\n",
      "         (biological, systems)       0.004819\n",
      "            (systems, through)       0.004854\n",
      "                (through, the)       0.004819\n",
      "              (the, framework)       0.004167\n",
      "               (framework, of)       0.004854\n",
      "           (of, connectionism)       0.004494\n",
      "       (connectionism, unlike)       0.004854\n",
      "                 (unlike, the)       0.004854\n",
      "        (model, connectionist)       0.004773\n",
      "    (connectionist, computing)       0.004854\n",
      "             (computing, does)       0.004854\n",
      "                   (does, not)       0.004854\n",
      "               (not, separate)       0.004831\n",
      "            (separate, memory)       0.004843\n",
      "                 (memory, and)       0.004843\n",
      "             (and, processing)       0.004598\n",
      "          (processing, warren)       0.004843\n",
      "           (warren, mcculloch)       0.004854\n",
      "              (mcculloch, and)       0.004854\n",
      "                 (and, walter)       0.004598\n",
      "               (walter, pitts)       0.004854\n",
      "           (pitts, considered)       0.004854\n",
      "               (considered, a)       0.004854\n",
      "              (a, nonlearning)       0.004535\n",
      "  (nonlearning, computational)       0.004854\n",
      "        (computational, model)       0.004831\n",
      "                  (model, for)       0.004773\n",
      "                 (for, neural)       0.004785\n",
      "              (networks, this)       0.004717\n",
      "                 (this, model)       0.004796\n",
      "                (model, paved)       0.004773\n",
      "                  (paved, the)       0.004854\n",
      "                    (the, way)       0.004167\n",
      "                    (way, for)       0.004854\n",
      "               (for, research)       0.007177\n",
      "                (research, to)       0.004819\n",
      "                   (to, split)       0.004619\n",
      "                 (split, into)       0.004854\n",
      "                   (into, two)       0.004843\n",
      "             (two, approaches)       0.004819\n",
      "             (approaches, one)       0.004854\n",
      "               (one, approach)       0.004843\n",
      "           (approach, focused)       0.004854\n",
      "                 (focused, on)       0.007264\n",
      "              (on, biological)       0.004762\n",
      "       (biological, processes)       0.004819\n",
      "            (processes, while)       0.004843\n",
      "                  (while, the)       0.004854\n",
      "              (other, focused)       0.004819\n",
      "            (the, application)       0.004167\n",
      "             (application, of)       0.004854\n",
      "                  (of, neural)       0.006742\n",
      "                (networks, to)       0.004717\n",
      "              (to, artificial)       0.004619\n",
      "            (intelligence, in)       0.004831\n",
      "                   (the, late)       0.004167\n",
      "                     (late, s)       0.004854\n",
      "                        (s, d)       0.004808\n",
      "                        (d, o)       0.004831\n",
      "                     (o, hebb)       0.004854\n",
      "              (hebb, proposed)       0.004854\n",
      "                 (proposed, a)       0.004854\n",
      "                 (a, learning)       0.004535\n",
      "        (learning, hypothesis)       0.004762\n",
      "           (hypothesis, based)       0.004854\n",
      "              (the, mechanism)       0.004167\n",
      "               (mechanism, of)       0.004854\n",
      "          (neural, plasticity)       0.004651\n",
      "            (plasticity, that)       0.004854\n",
      "                (that, became)       0.004843\n",
      "               (became, known)       0.004854\n",
      "                   (known, as)       0.004843\n",
      "                 (as, hebbian)       0.004785\n",
      "           (hebbian, learning)       0.004843\n",
      "                (learning, it)       0.004762\n",
      "                    (used, in)       0.004808\n",
      "                    (in, many)       0.004651\n",
      "                 (many, early)       0.004854\n",
      "               (early, neural)       0.004819\n",
      "              (networks, such)       0.004717\n",
      "             (as, rosenblatts)       0.004785\n",
      "     (rosenblatts, perceptron)       0.007264\n",
      "             (perceptron, and)       0.004808\n",
      "               (the, hopfield)       0.004167\n",
      "           (hopfield, network)       0.004854\n",
      "             (network, farley)       0.004717\n",
      "                 (farley, and)       0.009662\n",
      "                  (and, clark)       0.009195\n",
      "                 (clark, used)       0.004831\n",
      "         (used, computational)       0.004808\n",
      "     (computational, machines)       0.007246\n",
      "                (machines, to)       0.004843\n",
      "                (to, simulate)       0.004619\n",
      "                 (simulate, a)       0.004854\n",
      "                  (a, hebbian)       0.004535\n",
      "            (hebbian, network)       0.004843\n",
      "              (network, other)       0.004717\n",
      "               (other, neural)       0.004819\n",
      "      (network, computational)       0.004717\n",
      "              (machines, were)       0.004843\n",
      "               (were, created)       0.004843\n",
      "                 (created, by)       0.004854\n",
      "               (by, rochester)       0.004640\n",
      "          (rochester, holland)       0.004854\n",
      "              (holland, habit)       0.004854\n",
      "                  (habit, and)       0.004854\n",
      "                   (and, duda)       0.004598\n",
      "                    (duda, in)       0.004854\n",
      "            (in, psychologist)       0.004651\n",
      "         (psychologist, frank)       0.004854\n",
      "           (frank, rosenblatt)       0.004854\n",
      "       (rosenblatt, described)       0.004831\n",
      "              (described, the)       0.004843\n",
      "             (the, perceptron)       0.006250\n",
      "             (perceptron, one)       0.004808\n",
      "                     (one, of)       0.004843\n",
      "          (first, implemented)       0.004796\n",
      "     (implemented, artificial)       0.004854\n",
      "            (networks, funded)       0.004717\n",
      "                  (funded, by)       0.004854\n",
      "                 (the, united)       0.004167\n",
      "              (united, states)       0.004854\n",
      "              (states, office)       0.004854\n",
      "                  (office, of)       0.004854\n",
      "                   (of, naval)       0.004494\n",
      "             (naval, research)       0.004854\n",
      "                 (research, r)       0.004819\n",
      "                        (r, d)       0.004854\n",
      "                   (d, joseph)       0.004831\n",
      "            (joseph, mentions)       0.004843\n",
      "                (mentions, an)       0.004854\n",
      "                    (an, even)       0.004831\n",
      "               (even, earlier)       0.004854\n",
      "     (earlier, perceptronlike)       0.004854\n",
      "      (perceptronlike, device)       0.007264\n",
      "                  (device, by)       0.004843\n",
      "                  (by, farley)       0.004640\n",
      "               (clark, farley)       0.004831\n",
      "                   (clark, of)       0.004831\n",
      "                     (of, mit)       0.004494\n",
      "                (mit, lincoln)       0.004854\n",
      "         (lincoln, laboratory)       0.004854\n",
      "        (laboratory, actually)       0.004854\n",
      "          (actually, preceded)       0.004854\n",
      "        (preceded, rosenblatt)       0.004854\n",
      "              (rosenblatt, in)       0.004831\n",
      "            (the, development)       0.004167\n",
      "             (development, of)       0.004854\n",
      "           (a, perceptronlike)       0.004535\n",
      "             (device, however)       0.004843\n",
      "               (however, they)       0.004843\n",
      "               (they, dropped)       0.004831\n",
      "                (dropped, the)       0.004854\n",
      "                (the, subject)       0.004167\n",
      "                (subject, the)       0.004854\n",
      "          (perceptron, raised)       0.004808\n",
      "              (raised, public)       0.004854\n",
      "          (public, excitement)       0.004854\n",
      "             (excitement, for)       0.004854\n",
      "                (research, in)       0.004819\n",
      "           (networks, causing)       0.004717\n",
      "                (causing, the)       0.004854\n",
      "                     (the, us)       0.004167\n",
      "              (us, government)       0.004854\n",
      "              (government, to)       0.004854\n",
      "             (to, drastically)       0.004619\n",
      "       (drastically, increase)       0.004854\n",
      "           (increase, funding)       0.004854\n",
      "               (funding, this)       0.004854\n",
      "           (this, contributed)       0.004796\n",
      "             (contributed, to)       0.004854\n",
      "                 (the, golden)       0.004167\n",
      "                 (golden, age)       0.004854\n",
      "                     (age, of)       0.004854\n",
      "                      (of, ai)       0.004494\n",
      "                  (ai, fueled)       0.004854\n",
      "                  (fueled, by)       0.004854\n",
      "             (the, optimistic)       0.004167\n",
      "          (optimistic, claims)       0.004854\n",
      "                (claims, made)       0.004854\n",
      "                    (made, by)       0.004843\n",
      "                (by, computer)       0.004640\n",
      "        (computer, scientists)       0.004843\n",
      "       (scientists, regarding)       0.004854\n",
      "              (regarding, the)       0.004854\n",
      "                (the, ability)       0.004167\n",
      "                 (ability, of)       0.004854\n",
      "             (of, perceptrons)       0.004494\n",
      "             (perceptrons, to)       0.004831\n",
      "                 (to, emulate)       0.004619\n",
      "              (emulate, human)       0.004854\n",
      "         (human, intelligence)       0.004854\n",
      "           (intelligence, the)       0.004831\n",
      "          (first, perceptrons)       0.004796\n",
      "            (perceptrons, did)       0.004831\n",
      "                    (did, not)       0.007264\n",
      "                   (not, have)       0.004831\n",
      "              (have, adaptive)       0.004831\n",
      "            (adaptive, hidden)       0.007246\n",
      "               (hidden, units)       0.009592\n",
      "              (units, however)       0.004808\n",
      "             (however, joseph)       0.004843\n",
      "                (joseph, also)       0.004843\n",
      "             (also, discussed)       0.004808\n",
      "       (discussed, multilayer)       0.004854\n",
      "     (multilayer, perceptrons)       0.004843\n",
      "           (perceptrons, with)       0.004831\n",
      "                    (with, an)       0.004796\n",
      "                (an, adaptive)       0.004831\n",
      "               (hidden, layer)       0.004796\n",
      "           (layer, rosenblatt)       0.004762\n",
      "         (rosenblatt, section)       0.004831\n",
      "              (section, cited)       0.004854\n",
      "                  (cited, and)       0.004854\n",
      "                (and, adopted)       0.004598\n",
      "              (adopted, these)       0.004854\n",
      "                (these, ideas)       0.004808\n",
      "                 (ideas, also)       0.004854\n",
      "             (also, crediting)       0.004808\n",
      "             (crediting, work)       0.004854\n",
      "                    (work, by)       0.004831\n",
      "                       (by, h)       0.004640\n",
      "                        (h, d)       0.004854\n",
      "                    (d, block)       0.004831\n",
      "                  (block, and)       0.004854\n",
      "                      (and, b)       0.004598\n",
      "                        (b, w)       0.004854\n",
      "                   (w, knight)       0.004854\n",
      "       (knight, unfortunately)       0.004854\n",
      "        (unfortunately, these)       0.004854\n",
      "                (these, early)       0.004808\n",
      "              (early, efforts)       0.004819\n",
      "                (efforts, did)       0.004843\n",
      "                   (not, lead)       0.004831\n",
      "                    (lead, to)       0.004854\n",
      "                  (a, working)       0.004535\n",
      "           (working, learning)       0.004843\n",
      "         (learning, algorithm)       0.007143\n",
      "              (algorithm, for)       0.004843\n",
      "                 (for, hidden)       0.004785\n",
      "                   (units, ie)       0.004808\n",
      "                    (ie, deep)       0.004854\n",
      "              (deep, learning)       0.011905\n",
      "              (learning, deep)       0.004762\n",
      "     (learning, breakthroughs)       0.004762\n",
      "           (breakthroughs, in)       0.004854\n",
      "                      (the, s)       0.006250\n",
      "                      (s, and)       0.007212\n",
      "                      (and, s)       0.006897\n",
      "              (s, fundamental)       0.004808\n",
      "       (fundamental, research)       0.004854\n",
      "               (research, was)       0.004819\n",
      "              (was, conducted)       0.004808\n",
      "               (conducted, on)       0.004843\n",
      "                    (on, anns)       0.004762\n",
      "                    (anns, in)       0.004843\n",
      "                      (s, the)       0.004808\n",
      "              (first, working)       0.004796\n",
      "               (working, deep)       0.004843\n",
      "              (algorithm, was)       0.004843\n",
      "                    (was, the)       0.004808\n",
      "                  (the, group)       0.004167\n",
      "               (group, method)       0.004854\n",
      "                    (of, data)       0.004494\n",
      "              (data, handling)       0.004831\n",
      "                 (handling, a)       0.004854\n",
      "                   (a, method)       0.004535\n",
      "                  (method, to)       0.004796\n",
      "                   (to, train)       0.004619\n",
      "          (train, arbitrarily)       0.004854\n",
      "           (arbitrarily, deep)       0.004854\n",
      "         (networks, published)       0.004717\n",
      "               (published, by)       0.004843\n",
      "                  (by, alexey)       0.004640\n",
      "          (alexey, ivakhnenko)       0.004854\n",
      "             (ivakhnenko, and)       0.004854\n",
      "                   (and, lapa)       0.004598\n",
      "                    (lapa, in)       0.004854\n",
      "                 (the, soviet)       0.004167\n",
      "               (soviet, union)       0.004854\n",
      "                 (union, they)       0.004854\n",
      "              (they, regarded)       0.004831\n",
      "                (regarded, it)       0.004854\n",
      "                      (it, as)       0.004819\n",
      "                     (a, form)       0.004535\n",
      "                    (form, of)       0.004854\n",
      "              (of, polynomial)       0.004494\n",
      "      (polynomial, regression)       0.004854\n",
      "              (regression, or)       0.004831\n",
      "                       (or, a)       0.004785\n",
      "           (a, generalization)       0.004535\n",
      "          (generalization, of)       0.004854\n",
      "             (of, rosenblatts)       0.004494\n",
      "               (perceptron, a)       0.004808\n",
      "                    (a, paper)       0.004535\n",
      "            (paper, described)       0.004854\n",
      "                (described, a)       0.004843\n",
      "               (deep, network)       0.004762\n",
      "               (network, with)       0.004717\n",
      "                 (with, eight)       0.004796\n",
      "               (eight, layers)       0.004854\n",
      "             (layers, trained)       0.004785\n",
      "                 (trained, by)       0.007246\n",
      "                    (by, this)       0.004640\n",
      "               (method, which)       0.004796\n",
      "                   (which, is)       0.004808\n",
      "                   (on, layer)       0.004762\n",
      "                   (layer, by)       0.004762\n",
      "                   (by, layer)       0.004640\n",
      "             (layer, training)       0.004762\n",
      "           (training, through)       0.004819\n",
      "         (through, regression)       0.004819\n",
      "        (regression, analysis)       0.004831\n",
      "       (analysis, superfluous)       0.004854\n",
      "         (superfluous, hidden)       0.004854\n",
      "                  (units, are)       0.004808\n",
      "                 (are, pruned)       0.004751\n",
      "               (pruned, using)       0.004854\n",
      "                    (using, a)       0.004854\n",
      "                 (a, separate)       0.004535\n",
      "        (separate, validation)       0.004843\n",
      "             (validation, set)       0.004854\n",
      "                  (set, since)       0.004831\n",
      "                  (since, the)       0.004854\n",
      "               (functions, of)       0.004843\n",
      "                  (the, nodes)       0.004167\n",
      "                  (nodes, are)       0.004831\n",
      "        (are, kolmogorovgabor)       0.004751\n",
      "(kolmogorovgabor, polynomials)       0.004854\n",
      "          (polynomials, these)       0.004854\n",
      "                 (these, were)       0.004808\n",
      "                  (were, also)       0.004843\n",
      "                   (also, the)       0.004808\n",
      "                 (first, deep)       0.007194\n",
      "              (deep, networks)       0.004762\n",
      "              (networks, with)       0.004717\n",
      "        (with, multiplicative)       0.004796\n",
      "       (multiplicative, units)       0.004854\n",
      "                   (or, gates)       0.004785\n",
      "                  (gates, the)       0.004854\n",
      "        (learning, multilayer)       0.004762\n",
      "      (multilayer, perceptron)       0.004843\n",
      "         (perceptron, trained)       0.004808\n",
      "              (by, stochastic)       0.004640\n",
      "        (stochastic, gradient)       0.007264\n",
      "           (gradient, descent)       0.007264\n",
      "                (descent, was)       0.004843\n",
      "              (was, published)       0.004808\n",
      "               (published, in)       0.004843\n",
      "                      (in, by)       0.004651\n",
      "                (by, shunichi)       0.004640\n",
      "             (shunichi, amari)       0.004854\n",
      "                   (amari, in)       0.004854\n",
      "                (in, computer)       0.004651\n",
      "       (computer, experiments)       0.004843\n",
      "      (experiments, conducted)       0.004854\n",
      "               (conducted, by)       0.004843\n",
      "                  (by, amaris)       0.004640\n",
      "             (amaris, student)       0.004854\n",
      "              (student, saito)       0.004854\n",
      "                    (saito, a)       0.004854\n",
      "                     (a, five)       0.004535\n",
      "                 (five, layer)       0.004854\n",
      "                  (layer, mlp)       0.004762\n",
      "                   (mlp, with)       0.004854\n",
      "                   (with, two)       0.004796\n",
      "             (two, modifiable)       0.004819\n",
      "          (modifiable, layers)       0.004854\n",
      "             (layers, learned)       0.004785\n",
      "           (learned, internal)       0.004854\n",
      "   (internal, representations)       0.004854\n",
      "         (representations, to)       0.004854\n",
      "                (to, classify)       0.004619\n",
      "      (classify, nonlinearily)       0.004854\n",
      "     (nonlinearily, separable)       0.004854\n",
      "          (separable, pattern)       0.004854\n",
      "            (pattern, classes)       0.004854\n",
      "         (classes, subsequent)       0.004854\n",
      "    (subsequent, developments)       0.004854\n",
      "            (developments, in)       0.004854\n",
      "                (in, hardware)       0.004651\n",
      "               (hardware, and)       0.004854\n",
      "         (and, hyperparameter)       0.004598\n",
      "     (hyperparameter, tunings)       0.004854\n",
      "               (tunings, have)       0.004854\n",
      "                  (have, made)       0.004831\n",
      "              (made, endtoend)       0.004843\n",
      "        (endtoend, stochastic)       0.004854\n",
      "                (descent, the)       0.004843\n",
      "              (the, currently)       0.004167\n",
      "         (currently, dominant)       0.004854\n",
      "\n",
      "Trigram Probabilities:\n",
      "                              Trigram  Probability\n",
      "              (in, machine, learning)     0.004854\n",
      "               (machine, learning, a)     0.004854\n",
      "                (learning, a, neural)     0.004854\n",
      "                 (a, neural, network)     0.007264\n",
      "              (neural, network, also)     0.004796\n",
      "          (network, also, artificial)     0.004854\n",
      "           (also, artificial, neural)     0.004854\n",
      "        (artificial, neural, network)     0.004819\n",
      "                (neural, network, or)     0.004796\n",
      "                (network, or, neural)     0.004854\n",
      "                    (or, neural, net)     0.004854\n",
      "           (neural, net, abbreviated)     0.004854\n",
      "              (net, abbreviated, ann)     0.004854\n",
      "               (abbreviated, ann, or)     0.004854\n",
      "                        (ann, or, nn)     0.004854\n",
      "                         (or, nn, is)     0.004854\n",
      "                          (nn, is, a)     0.004854\n",
      "                       (is, a, model)     0.004831\n",
      "                 (a, model, inspired)     0.004854\n",
      "                (model, inspired, by)     0.004854\n",
      "                  (inspired, by, the)     0.004854\n",
      "                 (by, the, structure)     0.004831\n",
      "                (the, structure, and)     0.004854\n",
      "           (structure, and, function)     0.004854\n",
      "                  (and, function, of)     0.004854\n",
      "           (function, of, biological)     0.004843\n",
      "             (of, biological, neural)     0.004854\n",
      "       (biological, neural, networks)     0.004854\n",
      "               (neural, networks, in)     0.004739\n",
      "               (networks, in, animal)     0.004854\n",
      "                 (in, animal, brains)     0.004854\n",
      "                  (animal, brains, a)     0.004854\n",
      "                  (brains, a, neural)     0.004854\n",
      "          (neural, network, consists)     0.004796\n",
      "              (network, consists, of)     0.004854\n",
      "            (consists, of, connected)     0.004843\n",
      "               (of, connected, units)     0.004854\n",
      "               (connected, units, or)     0.004854\n",
      "                   (units, or, nodes)     0.004843\n",
      "                  (or, nodes, called)     0.004854\n",
      "          (nodes, called, artificial)     0.004854\n",
      "        (called, artificial, neurons)     0.004854\n",
      "         (artificial, neurons, which)     0.004854\n",
      "            (neurons, which, loosely)     0.004854\n",
      "              (which, loosely, model)     0.004854\n",
      "                (loosely, model, the)     0.004854\n",
      "                (model, the, neurons)     0.004843\n",
      "                   (the, neurons, in)     0.004854\n",
      "                   (neurons, in, the)     0.004854\n",
      "                     (in, the, brain)     0.007177\n",
      "             (the, brain, artificial)     0.004843\n",
      "          (brain, artificial, neuron)     0.004854\n",
      "         (artificial, neuron, models)     0.004843\n",
      "               (neuron, models, that)     0.004854\n",
      "                (models, that, mimic)     0.004854\n",
      "            (that, mimic, biological)     0.004854\n",
      "         (mimic, biological, neurons)     0.004854\n",
      "          (biological, neurons, more)     0.004854\n",
      "             (neurons, more, closely)     0.004854\n",
      "                (more, closely, have)     0.004854\n",
      "                (closely, have, also)     0.004854\n",
      "                   (have, also, been)     0.004854\n",
      "               (also, been, recently)     0.004854\n",
      "       (been, recently, investigated)     0.004854\n",
      "        (recently, investigated, and)     0.004854\n",
      "           (investigated, and, shown)     0.004854\n",
      "                     (and, shown, to)     0.004854\n",
      "           (shown, to, significantly)     0.004854\n",
      "         (to, significantly, improve)     0.004854\n",
      "(significantly, improve, performance)     0.004854\n",
      "        (improve, performance, these)     0.004854\n",
      "            (performance, these, are)     0.004854\n",
      "              (these, are, connected)     0.004854\n",
      "                 (are, connected, by)     0.004854\n",
      "               (connected, by, edges)     0.004854\n",
      "                   (by, edges, which)     0.004854\n",
      "                (edges, which, model)     0.004854\n",
      "                  (which, model, the)     0.004854\n",
      "               (model, the, synapses)     0.004843\n",
      "                  (the, synapses, in)     0.004854\n",
      "                  (synapses, in, the)     0.004854\n",
      "                   (the, brain, each)     0.004843\n",
      "            (brain, each, artificial)     0.004854\n",
      "           (each, artificial, neuron)     0.004854\n",
      "       (artificial, neuron, receives)     0.004843\n",
      "          (neuron, receives, signals)     0.004854\n",
      "            (receives, signals, from)     0.004854\n",
      "           (signals, from, connected)     0.004854\n",
      "           (from, connected, neurons)     0.004854\n",
      "           (connected, neurons, then)     0.004843\n",
      "           (neurons, then, processes)     0.004854\n",
      "              (then, processes, them)     0.004854\n",
      "               (processes, them, and)     0.004854\n",
      "                   (them, and, sends)     0.004854\n",
      "                      (and, sends, a)     0.004854\n",
      "                   (sends, a, signal)     0.004854\n",
      "                      (a, signal, to)     0.004854\n",
      "                  (signal, to, other)     0.004854\n",
      "               (to, other, connected)     0.004854\n",
      "          (other, connected, neurons)     0.004854\n",
      "            (connected, neurons, the)     0.004843\n",
      "               (neurons, the, signal)     0.004854\n",
      "                    (the, signal, is)     0.004843\n",
      "                      (signal, is, a)     0.004854\n",
      "                        (is, a, real)     0.004831\n",
      "                    (a, real, number)     0.004854\n",
      "                  (real, number, and)     0.004854\n",
      "                   (number, and, the)     0.004854\n",
      "                   (and, the, output)     0.004808\n",
      "                    (the, output, of)     0.004843\n",
      "                   (output, of, each)     0.004854\n",
      "                   (of, each, neuron)     0.004854\n",
      "                   (each, neuron, is)     0.004854\n",
      "               (neuron, is, computed)     0.004854\n",
      "                   (is, computed, by)     0.004854\n",
      "                 (computed, by, some)     0.004854\n",
      "                (by, some, nonlinear)     0.004854\n",
      "          (some, nonlinear, function)     0.004854\n",
      "            (nonlinear, function, of)     0.004854\n",
      "                  (function, of, the)     0.004843\n",
      "                       (of, the, sum)     0.004785\n",
      "                       (the, sum, of)     0.007264\n",
      "                       (sum, of, its)     0.004843\n",
      "                    (of, its, inputs)     0.004854\n",
      "                (its, inputs, called)     0.004854\n",
      "                (inputs, called, the)     0.004854\n",
      "            (called, the, activation)     0.004854\n",
      "          (the, activation, function)     0.004843\n",
      "          (activation, function, the)     0.004854\n",
      "            (function, the, strength)     0.004854\n",
      "                  (the, strength, of)     0.004854\n",
      "                  (strength, of, the)     0.004854\n",
      "                    (of, the, signal)     0.004785\n",
      "                    (the, signal, at)     0.004843\n",
      "                   (signal, at, each)     0.004854\n",
      "               (at, each, connection)     0.004843\n",
      "               (each, connection, is)     0.004854\n",
      "         (connection, is, determined)     0.004854\n",
      "                 (is, determined, by)     0.004854\n",
      "                  (determined, by, a)     0.004854\n",
      "                      (by, a, weight)     0.004843\n",
      "                   (a, weight, which)     0.004854\n",
      "             (weight, which, adjusts)     0.004854\n",
      "             (which, adjusts, during)     0.004854\n",
      "               (adjusts, during, the)     0.004854\n",
      "              (during, the, learning)     0.004843\n",
      "             (the, learning, process)     0.004854\n",
      "       (learning, process, typically)     0.004854\n",
      "        (process, typically, neurons)     0.004854\n",
      "            (typically, neurons, are)     0.004854\n",
      "           (neurons, are, aggregated)     0.004854\n",
      "              (are, aggregated, into)     0.004854\n",
      "           (aggregated, into, layers)     0.004854\n",
      "            (into, layers, different)     0.004854\n",
      "          (layers, different, layers)     0.004854\n",
      "             (different, layers, may)     0.004854\n",
      "               (layers, may, perform)     0.004854\n",
      "            (may, perform, different)     0.004854\n",
      "(perform, different, transformations)     0.004854\n",
      "     (different, transformations, on)     0.004854\n",
      "         (transformations, on, their)     0.004854\n",
      "                  (on, their, inputs)     0.004854\n",
      "             (their, inputs, signals)     0.004854\n",
      "            (inputs, signals, travel)     0.004854\n",
      "              (signals, travel, from)     0.004854\n",
      "                  (travel, from, the)     0.004854\n",
      "                   (from, the, first)     0.004854\n",
      "                  (the, first, layer)     0.004796\n",
      "                  (first, layer, the)     0.004854\n",
      "                  (layer, the, input)     0.004843\n",
      "                  (the, input, layer)     0.004854\n",
      "                   (input, layer, to)     0.004854\n",
      "                     (layer, to, the)     0.004854\n",
      "                      (to, the, last)     0.004819\n",
      "                   (the, last, layer)     0.004854\n",
      "                   (last, layer, the)     0.004854\n",
      "                 (layer, the, output)     0.004843\n",
      "                 (the, output, layer)     0.004843\n",
      "            (output, layer, possibly)     0.004854\n",
      "           (layer, possibly, passing)     0.004854\n",
      "         (possibly, passing, through)     0.004854\n",
      "         (passing, through, multiple)     0.004854\n",
      "    (through, multiple, intermediate)     0.004854\n",
      "     (multiple, intermediate, layers)     0.004854\n",
      "       (intermediate, layers, hidden)     0.004854\n",
      "             (layers, hidden, layers)     0.004854\n",
      "                  (hidden, layers, a)     0.004843\n",
      "                 (layers, a, network)     0.004854\n",
      "                     (a, network, is)     0.004854\n",
      "             (network, is, typically)     0.004854\n",
      "              (is, typically, called)     0.004854\n",
      "               (typically, called, a)     0.004854\n",
      "                    (called, a, deep)     0.004854\n",
      "                    (a, deep, neural)     0.004843\n",
      "              (deep, neural, network)     0.004831\n",
      "                (neural, network, if)     0.004796\n",
      "                    (network, if, it)     0.004854\n",
      "                        (if, it, has)     0.004854\n",
      "                        (it, has, at)     0.004854\n",
      "                     (has, at, least)     0.004854\n",
      "                     (at, least, two)     0.004854\n",
      "                 (least, two, hidden)     0.004854\n",
      "                (two, hidden, layers)     0.004854\n",
      "         (hidden, layers, artificial)     0.004843\n",
      "         (layers, artificial, neural)     0.004854\n",
      "       (artificial, neural, networks)     0.009639\n",
      "              (neural, networks, are)     0.009479\n",
      "                (networks, are, used)     0.004831\n",
      "                     (are, used, for)     0.004854\n",
      "                 (used, for, various)     0.004854\n",
      "                (for, various, tasks)     0.004854\n",
      "          (various, tasks, including)     0.004854\n",
      "       (tasks, including, predictive)     0.004854\n",
      "    (including, predictive, modeling)     0.004854\n",
      "     (predictive, modeling, adaptive)     0.004854\n",
      "        (modeling, adaptive, control)     0.004854\n",
      "             (adaptive, control, and)     0.004854\n",
      "              (control, and, solving)     0.004854\n",
      "             (and, solving, problems)     0.004854\n",
      "              (solving, problems, in)     0.004854\n",
      "           (problems, in, artificial)     0.004854\n",
      "       (in, artificial, intelligence)     0.004843\n",
      "     (artificial, intelligence, they)     0.004843\n",
      "            (intelligence, they, can)     0.004854\n",
      "                   (they, can, learn)     0.004854\n",
      "                   (can, learn, from)     0.004854\n",
      "            (learn, from, experience)     0.004843\n",
      "              (from, experience, and)     0.004854\n",
      "               (experience, and, can)     0.004854\n",
      "                   (and, can, derive)     0.004854\n",
      "           (can, derive, conclusions)     0.004854\n",
      "          (derive, conclusions, from)     0.004854\n",
      "               (conclusions, from, a)     0.004854\n",
      "                   (from, a, complex)     0.004854\n",
      "                    (a, complex, and)     0.004854\n",
      "            (complex, and, seemingly)     0.004854\n",
      "          (and, seemingly, unrelated)     0.004854\n",
      "          (seemingly, unrelated, set)     0.004854\n",
      "                 (unrelated, set, of)     0.004854\n",
      "               (set, of, information)     0.004843\n",
      "          (of, information, training)     0.004854\n",
      "      (information, training, neural)     0.004854\n",
      "         (training, neural, networks)     0.004854\n",
      "           (networks, are, typically)     0.004831\n",
      "            (are, typically, trained)     0.004854\n",
      "        (typically, trained, through)     0.004854\n",
      "        (trained, through, empirical)     0.004854\n",
      "           (through, empirical, risk)     0.004854\n",
      "      (empirical, risk, minimization)     0.004843\n",
      "           (risk, minimization, this)     0.004854\n",
      "         (minimization, this, method)     0.004854\n",
      "                   (this, method, is)     0.004831\n",
      "                  (method, is, based)     0.004854\n",
      "                      (is, based, on)     0.007264\n",
      "                     (based, on, the)     0.007229\n",
      "                      (on, the, idea)     0.004819\n",
      "                      (the, idea, of)     0.004854\n",
      "               (idea, of, optimizing)     0.004854\n",
      "                (of, optimizing, the)     0.004854\n",
      "          (optimizing, the, networks)     0.004854\n",
      "          (the, networks, parameters)     0.004854\n",
      "           (networks, parameters, to)     0.004854\n",
      "           (parameters, to, minimize)     0.007264\n",
      "                  (to, minimize, the)     0.004843\n",
      "          (minimize, the, difference)     0.004854\n",
      "                (the, difference, or)     0.004854\n",
      "          (difference, or, empirical)     0.004854\n",
      "                (or, empirical, risk)     0.004854\n",
      "           (empirical, risk, between)     0.004843\n",
      "                 (risk, between, the)     0.004854\n",
      "            (between, the, predicted)     0.004854\n",
      "             (the, predicted, output)     0.004854\n",
      "             (predicted, output, and)     0.004854\n",
      "                   (output, and, the)     0.004854\n",
      "                   (and, the, actual)     0.004808\n",
      "                (the, actual, target)     0.004854\n",
      "             (actual, target, values)     0.004854\n",
      "                 (target, values, in)     0.004843\n",
      "                      (values, in, a)     0.004854\n",
      "                       (in, a, given)     0.004854\n",
      "                  (a, given, dataset)     0.004854\n",
      "      (given, dataset, gradientbased)     0.004854\n",
      "    (dataset, gradientbased, methods)     0.004854\n",
      "       (gradientbased, methods, such)     0.004854\n",
      "                  (methods, such, as)     0.004854\n",
      "          (such, as, backpropagation)     0.004831\n",
      "           (as, backpropagation, are)     0.004854\n",
      "      (backpropagation, are, usually)     0.004854\n",
      "                 (are, usually, used)     0.004854\n",
      "                  (usually, used, to)     0.004854\n",
      "                 (used, to, estimate)     0.004854\n",
      "                  (to, estimate, the)     0.004854\n",
      "          (estimate, the, parameters)     0.004854\n",
      "                (the, parameters, of)     0.004854\n",
      "                (parameters, of, the)     0.004854\n",
      "                   (of, the, network)     0.004785\n",
      "               (the, network, during)     0.004843\n",
      "               (network, during, the)     0.004854\n",
      "              (during, the, training)     0.004843\n",
      "               (the, training, phase)     0.004854\n",
      "              (training, phase, anns)     0.004854\n",
      "                 (phase, anns, learn)     0.004854\n",
      "                  (anns, learn, from)     0.004854\n",
      "               (learn, from, labeled)     0.004843\n",
      "            (from, labeled, training)     0.004854\n",
      "            (labeled, training, data)     0.004854\n",
      "                 (training, data, by)     0.004854\n",
      "              (data, by, iteratively)     0.004854\n",
      "          (by, iteratively, updating)     0.004854\n",
      "       (iteratively, updating, their)     0.004854\n",
      "        (updating, their, parameters)     0.004854\n",
      "              (their, parameters, to)     0.004854\n",
      "                    (to, minimize, a)     0.004843\n",
      "               (minimize, a, defined)     0.004854\n",
      "                   (a, defined, loss)     0.004854\n",
      "            (defined, loss, function)     0.004854\n",
      "               (loss, function, this)     0.004854\n",
      "             (function, this, method)     0.004854\n",
      "               (this, method, allows)     0.004831\n",
      "                (method, allows, the)     0.004854\n",
      "               (allows, the, network)     0.004854\n",
      "                   (the, network, to)     0.004843\n",
      "            (network, to, generalize)     0.004854\n",
      "                 (to, generalize, to)     0.004854\n",
      "             (generalize, to, unseen)     0.004854\n",
      "                   (to, unseen, data)     0.004854\n",
      "              (unseen, data, history)     0.004854\n",
      "               (data, history, early)     0.004854\n",
      "               (history, early, work)     0.004854\n",
      "                (early, work, todays)     0.004843\n",
      "                 (work, todays, deep)     0.004854\n",
      "               (todays, deep, neural)     0.004854\n",
      "             (deep, neural, networks)     0.007246\n",
      "               (networks, are, based)     0.004831\n",
      "                     (are, based, on)     0.004854\n",
      "                   (based, on, early)     0.004819\n",
      "                    (on, early, work)     0.004854\n",
      "                    (early, work, in)     0.004843\n",
      "               (work, in, statistics)     0.004854\n",
      "               (in, statistics, over)     0.004854\n",
      "            (statistics, over, years)     0.004854\n",
      "                   (over, years, ago)     0.004854\n",
      "                    (years, ago, the)     0.004854\n",
      "                 (ago, the, simplest)     0.004854\n",
      "                (the, simplest, kind)     0.004854\n",
      "                 (simplest, kind, of)     0.004854\n",
      "              (kind, of, feedforward)     0.004854\n",
      "            (of, feedforward, neural)     0.004854\n",
      "       (feedforward, neural, network)     0.004854\n",
      "               (neural, network, fnn)     0.004796\n",
      "                   (network, fnn, is)     0.004854\n",
      "                         (fnn, is, a)     0.004854\n",
      "                      (is, a, linear)     0.004831\n",
      "                 (a, linear, network)     0.004854\n",
      "             (linear, network, which)     0.004854\n",
      "           (network, which, consists)     0.004854\n",
      "                (which, consists, of)     0.004854\n",
      "                    (consists, of, a)     0.004843\n",
      "                      (of, a, single)     0.004843\n",
      "                   (a, single, layer)     0.004854\n",
      "                  (single, layer, of)     0.004854\n",
      "                  (layer, of, output)     0.004854\n",
      "                  (of, output, nodes)     0.004854\n",
      "                (output, nodes, with)     0.004854\n",
      "                (nodes, with, linear)     0.004854\n",
      "           (with, linear, activation)     0.004854\n",
      "      (linear, activation, functions)     0.004854\n",
      "         (activation, functions, the)     0.004843\n",
      "             (functions, the, inputs)     0.004854\n",
      "                   (the, inputs, are)     0.004843\n",
      "                   (inputs, are, fed)     0.004854\n",
      "                 (are, fed, directly)     0.004854\n",
      "                  (fed, directly, to)     0.004854\n",
      "                  (directly, to, the)     0.004854\n",
      "                   (to, the, outputs)     0.004819\n",
      "                  (the, outputs, via)     0.004854\n",
      "                    (outputs, via, a)     0.004854\n",
      "                     (via, a, series)     0.004854\n",
      "                      (a, series, of)     0.004854\n",
      "                (series, of, weights)     0.004854\n",
      "                   (of, weights, the)     0.004854\n",
      "                  (weights, the, sum)     0.004854\n",
      "                       (sum, of, the)     0.004843\n",
      "                  (of, the, products)     0.004785\n",
      "                  (the, products, of)     0.004854\n",
      "                  (products, of, the)     0.004854\n",
      "                   (of, the, weights)     0.004785\n",
      "                  (the, weights, and)     0.004843\n",
      "                  (weights, and, the)     0.004854\n",
      "                   (and, the, inputs)     0.004808\n",
      "                    (the, inputs, is)     0.004843\n",
      "             (inputs, is, calculated)     0.004854\n",
      "                 (is, calculated, at)     0.004854\n",
      "               (calculated, at, each)     0.004854\n",
      "                     (at, each, node)     0.004843\n",
      "                    (each, node, the)     0.004854\n",
      "                    (node, the, mean)     0.004854\n",
      "                 (the, mean, squared)     0.004854\n",
      "              (mean, squared, errors)     0.004854\n",
      "           (squared, errors, between)     0.004854\n",
      "             (errors, between, these)     0.004854\n",
      "         (between, these, calculated)     0.004854\n",
      "         (these, calculated, outputs)     0.004854\n",
      "           (calculated, outputs, and)     0.004854\n",
      "                  (outputs, and, the)     0.004854\n",
      "                    (and, the, given)     0.004808\n",
      "                 (the, given, target)     0.004854\n",
      "              (given, target, values)     0.004854\n",
      "                (target, values, are)     0.004843\n",
      "             (values, are, minimized)     0.004854\n",
      "                 (are, minimized, by)     0.004854\n",
      "            (minimized, by, creating)     0.004854\n",
      "                   (by, creating, an)     0.004854\n",
      "           (creating, an, adjustment)     0.004854\n",
      "                 (an, adjustment, to)     0.004854\n",
      "                (adjustment, to, the)     0.004854\n",
      "                   (to, the, weights)     0.004819\n",
      "                 (the, weights, this)     0.004843\n",
      "           (weights, this, technique)     0.004854\n",
      "               (this, technique, has)     0.004854\n",
      "               (technique, has, been)     0.004854\n",
      "                   (has, been, known)     0.004854\n",
      "                   (been, known, for)     0.004854\n",
      "                   (known, for, over)     0.004854\n",
      "                     (for, over, two)     0.004854\n",
      "               (over, two, centuries)     0.004854\n",
      "                 (two, centuries, as)     0.004854\n",
      "                 (centuries, as, the)     0.004854\n",
      "                    (as, the, method)     0.004843\n",
      "                    (the, method, of)     0.004854\n",
      "                  (method, of, least)     0.004843\n",
      "                 (of, least, squares)     0.004854\n",
      "                 (least, squares, or)     0.004854\n",
      "                (squares, or, linear)     0.004854\n",
      "             (or, linear, regression)     0.004854\n",
      "             (linear, regression, it)     0.004854\n",
      "                (regression, it, was)     0.004854\n",
      "                      (it, was, used)     0.007264\n",
      "                      (was, used, as)     0.004843\n",
      "                        (used, as, a)     0.004854\n",
      "                       (as, a, means)     0.004843\n",
      "                       (a, means, of)     0.004854\n",
      "                 (means, of, finding)     0.004854\n",
      "                     (of, finding, a)     0.004854\n",
      "                   (finding, a, good)     0.004854\n",
      "                     (a, good, rough)     0.004854\n",
      "                (good, rough, linear)     0.004854\n",
      "                 (rough, linear, fit)     0.004854\n",
      "                    (linear, fit, to)     0.004854\n",
      "                         (fit, to, a)     0.004854\n",
      "                         (to, a, set)     0.004843\n",
      "                         (a, set, of)     0.004854\n",
      "                    (set, of, points)     0.004843\n",
      "                     (of, points, by)     0.004854\n",
      "               (points, by, legendre)     0.004854\n",
      "                  (by, legendre, and)     0.004854\n",
      "               (legendre, and, gauss)     0.004854\n",
      "                    (and, gauss, for)     0.004854\n",
      "                    (gauss, for, the)     0.004854\n",
      "               (for, the, prediction)     0.004854\n",
      "                (the, prediction, of)     0.004854\n",
      "          (prediction, of, planetary)     0.004854\n",
      "            (of, planetary, movement)     0.004854\n",
      "  (planetary, movement, historically)     0.004854\n",
      "    (movement, historically, digital)     0.004854\n",
      "   (historically, digital, computers)     0.004854\n",
      "           (digital, computers, such)     0.004854\n",
      "                (computers, such, as)     0.004854\n",
      "                      (such, as, the)     0.004831\n",
      "                       (as, the, von)     0.004843\n",
      "                  (the, von, neumann)     0.007264\n",
      "                (von, neumann, model)     0.007264\n",
      "            (neumann, model, operate)     0.004843\n",
      "                (model, operate, via)     0.004854\n",
      "                  (operate, via, the)     0.004854\n",
      "                (via, the, execution)     0.004854\n",
      "                 (the, execution, of)     0.004854\n",
      "            (execution, of, explicit)     0.004854\n",
      "         (of, explicit, instructions)     0.004854\n",
      "       (explicit, instructions, with)     0.004854\n",
      "         (instructions, with, access)     0.004854\n",
      "                   (with, access, to)     0.004854\n",
      "                 (access, to, memory)     0.004854\n",
      "                     (to, memory, by)     0.004854\n",
      "                      (memory, by, a)     0.004854\n",
      "                      (by, a, number)     0.004843\n",
      "                      (a, number, of)     0.004854\n",
      "             (number, of, processors)     0.004854\n",
      "               (of, processors, some)     0.004854\n",
      "           (processors, some, neural)     0.004854\n",
      "             (some, neural, networks)     0.004854\n",
      "               (neural, networks, on)     0.004739\n",
      "                  (networks, on, the)     0.004854\n",
      "                     (on, the, other)     0.004819\n",
      "                   (the, other, hand)     0.004843\n",
      "            (other, hand, originated)     0.004854\n",
      "             (hand, originated, from)     0.004854\n",
      "          (originated, from, efforts)     0.004854\n",
      "                  (from, efforts, to)     0.004854\n",
      "                 (efforts, to, model)     0.004854\n",
      "             (to, model, information)     0.004854\n",
      "     (model, information, processing)     0.004854\n",
      "        (information, processing, in)     0.004854\n",
      "         (processing, in, biological)     0.004854\n",
      "            (in, biological, systems)     0.004854\n",
      "       (biological, systems, through)     0.004854\n",
      "              (systems, through, the)     0.004854\n",
      "            (through, the, framework)     0.004854\n",
      "                 (the, framework, of)     0.004854\n",
      "       (framework, of, connectionism)     0.004854\n",
      "          (of, connectionism, unlike)     0.004854\n",
      "         (connectionism, unlike, the)     0.004854\n",
      "                   (unlike, the, von)     0.004854\n",
      "      (neumann, model, connectionist)     0.004843\n",
      "    (model, connectionist, computing)     0.004854\n",
      "     (connectionist, computing, does)     0.004854\n",
      "               (computing, does, not)     0.004854\n",
      "                (does, not, separate)     0.004854\n",
      "              (not, separate, memory)     0.004854\n",
      "              (separate, memory, and)     0.004854\n",
      "            (memory, and, processing)     0.004854\n",
      "            (and, processing, warren)     0.004854\n",
      "      (processing, warren, mcculloch)     0.004854\n",
      "             (warren, mcculloch, and)     0.004854\n",
      "             (mcculloch, and, walter)     0.004854\n",
      "                 (and, walter, pitts)     0.004854\n",
      "          (walter, pitts, considered)     0.004854\n",
      "               (pitts, considered, a)     0.004854\n",
      "         (considered, a, nonlearning)     0.004854\n",
      "      (a, nonlearning, computational)     0.004854\n",
      "  (nonlearning, computational, model)     0.004854\n",
      "          (computational, model, for)     0.004854\n",
      "                 (model, for, neural)     0.004854\n",
      "              (for, neural, networks)     0.004854\n",
      "             (neural, networks, this)     0.004739\n",
      "              (networks, this, model)     0.004854\n",
      "                 (this, model, paved)     0.004854\n",
      "                  (model, paved, the)     0.004854\n",
      "                    (paved, the, way)     0.004854\n",
      "                      (the, way, for)     0.004854\n",
      "                 (way, for, research)     0.004854\n",
      "                  (for, research, to)     0.004843\n",
      "                (research, to, split)     0.004854\n",
      "                    (to, split, into)     0.004854\n",
      "                   (split, into, two)     0.004854\n",
      "              (into, two, approaches)     0.004854\n",
      "               (two, approaches, one)     0.004854\n",
      "          (approaches, one, approach)     0.004854\n",
      "             (one, approach, focused)     0.004854\n",
      "              (approach, focused, on)     0.004854\n",
      "            (focused, on, biological)     0.004843\n",
      "          (on, biological, processes)     0.004854\n",
      "       (biological, processes, while)     0.004854\n",
      "              (processes, while, the)     0.004854\n",
      "                  (while, the, other)     0.004854\n",
      "                (the, other, focused)     0.004843\n",
      "                 (other, focused, on)     0.004854\n",
      "                   (focused, on, the)     0.004843\n",
      "               (on, the, application)     0.004819\n",
      "               (the, application, of)     0.004854\n",
      "            (application, of, neural)     0.004854\n",
      "               (of, neural, networks)     0.004843\n",
      "               (neural, networks, to)     0.004739\n",
      "           (networks, to, artificial)     0.004854\n",
      "       (to, artificial, intelligence)     0.004854\n",
      "       (artificial, intelligence, in)     0.004843\n",
      "              (intelligence, in, the)     0.004854\n",
      "                      (in, the, late)     0.004785\n",
      "                       (the, late, s)     0.004854\n",
      "                         (late, s, d)     0.004854\n",
      "                            (s, d, o)     0.004854\n",
      "                         (d, o, hebb)     0.004854\n",
      "                  (o, hebb, proposed)     0.004854\n",
      "                  (hebb, proposed, a)     0.004854\n",
      "              (proposed, a, learning)     0.004854\n",
      "            (a, learning, hypothesis)     0.004854\n",
      "        (learning, hypothesis, based)     0.004854\n",
      "              (hypothesis, based, on)     0.004854\n",
      "                 (on, the, mechanism)     0.004819\n",
      "                 (the, mechanism, of)     0.004854\n",
      "              (mechanism, of, neural)     0.004854\n",
      "             (of, neural, plasticity)     0.004843\n",
      "           (neural, plasticity, that)     0.004854\n",
      "           (plasticity, that, became)     0.004854\n",
      "                (that, became, known)     0.004854\n",
      "                  (became, known, as)     0.004854\n",
      "                 (known, as, hebbian)     0.004854\n",
      "              (as, hebbian, learning)     0.004854\n",
      "              (hebbian, learning, it)     0.004854\n",
      "                  (learning, it, was)     0.004854\n",
      "                      (was, used, in)     0.004843\n",
      "                     (used, in, many)     0.004854\n",
      "                    (in, many, early)     0.004854\n",
      "                (many, early, neural)     0.004854\n",
      "            (early, neural, networks)     0.004854\n",
      "             (neural, networks, such)     0.004739\n",
      "                 (networks, such, as)     0.004854\n",
      "              (such, as, rosenblatts)     0.004831\n",
      "        (as, rosenblatts, perceptron)     0.004854\n",
      "       (rosenblatts, perceptron, and)     0.004843\n",
      "               (perceptron, and, the)     0.004854\n",
      "                 (and, the, hopfield)     0.004808\n",
      "             (the, hopfield, network)     0.004854\n",
      "          (hopfield, network, farley)     0.004854\n",
      "               (network, farley, and)     0.004854\n",
      "                 (farley, and, clark)     0.009662\n",
      "                   (and, clark, used)     0.004831\n",
      "         (clark, used, computational)     0.004854\n",
      "      (used, computational, machines)     0.004854\n",
      "        (computational, machines, to)     0.004843\n",
      "             (machines, to, simulate)     0.004854\n",
      "                    (to, simulate, a)     0.004854\n",
      "               (simulate, a, hebbian)     0.004854\n",
      "                (a, hebbian, network)     0.004854\n",
      "            (hebbian, network, other)     0.004854\n",
      "             (network, other, neural)     0.004854\n",
      "             (other, neural, network)     0.004854\n",
      "     (neural, network, computational)     0.004796\n",
      "   (network, computational, machines)     0.004854\n",
      "      (computational, machines, were)     0.004843\n",
      "            (machines, were, created)     0.004854\n",
      "                  (were, created, by)     0.004854\n",
      "             (created, by, rochester)     0.004854\n",
      "             (by, rochester, holland)     0.004854\n",
      "          (rochester, holland, habit)     0.004854\n",
      "                (holland, habit, and)     0.004854\n",
      "                   (habit, and, duda)     0.004854\n",
      "                      (and, duda, in)     0.004854\n",
      "             (duda, in, psychologist)     0.004854\n",
      "            (in, psychologist, frank)     0.004854\n",
      "    (psychologist, frank, rosenblatt)     0.004854\n",
      "       (frank, rosenblatt, described)     0.004854\n",
      "         (rosenblatt, described, the)     0.004854\n",
      "         (described, the, perceptron)     0.004854\n",
      "               (the, perceptron, one)     0.004843\n",
      "                (perceptron, one, of)     0.004854\n",
      "                       (one, of, the)     0.004854\n",
      "                     (of, the, first)     0.004785\n",
      "            (the, first, implemented)     0.004796\n",
      "     (first, implemented, artificial)     0.004854\n",
      "    (implemented, artificial, neural)     0.004854\n",
      "           (neural, networks, funded)     0.004739\n",
      "               (networks, funded, by)     0.004854\n",
      "                    (funded, by, the)     0.004854\n",
      "                    (by, the, united)     0.004831\n",
      "                (the, united, states)     0.004854\n",
      "             (united, states, office)     0.004854\n",
      "                 (states, office, of)     0.004854\n",
      "                  (office, of, naval)     0.004854\n",
      "                (of, naval, research)     0.004854\n",
      "                 (naval, research, r)     0.004854\n",
      "                     (research, r, d)     0.004854\n",
      "                       (r, d, joseph)     0.004854\n",
      "                (d, joseph, mentions)     0.004854\n",
      "               (joseph, mentions, an)     0.004854\n",
      "                 (mentions, an, even)     0.004854\n",
      "                  (an, even, earlier)     0.004854\n",
      "      (even, earlier, perceptronlike)     0.004854\n",
      "    (earlier, perceptronlike, device)     0.004854\n",
      "         (perceptronlike, device, by)     0.004843\n",
      "                 (device, by, farley)     0.004854\n",
      "                    (by, farley, and)     0.004854\n",
      "                 (and, clark, farley)     0.004831\n",
      "                 (clark, farley, and)     0.004854\n",
      "                     (and, clark, of)     0.004831\n",
      "                     (clark, of, mit)     0.004854\n",
      "                   (of, mit, lincoln)     0.004854\n",
      "           (mit, lincoln, laboratory)     0.004854\n",
      "      (lincoln, laboratory, actually)     0.004854\n",
      "     (laboratory, actually, preceded)     0.004854\n",
      "     (actually, preceded, rosenblatt)     0.004854\n",
      "           (preceded, rosenblatt, in)     0.004854\n",
      "                (rosenblatt, in, the)     0.004854\n",
      "               (in, the, development)     0.004785\n",
      "               (the, development, of)     0.004854\n",
      "                 (development, of, a)     0.004854\n",
      "              (of, a, perceptronlike)     0.004843\n",
      "          (a, perceptronlike, device)     0.004854\n",
      "    (perceptronlike, device, however)     0.004843\n",
      "              (device, however, they)     0.004854\n",
      "             (however, they, dropped)     0.004854\n",
      "                 (they, dropped, the)     0.004854\n",
      "              (dropped, the, subject)     0.004854\n",
      "                  (the, subject, the)     0.004854\n",
      "           (subject, the, perceptron)     0.004854\n",
      "            (the, perceptron, raised)     0.004843\n",
      "         (perceptron, raised, public)     0.004854\n",
      "         (raised, public, excitement)     0.004854\n",
      "            (public, excitement, for)     0.004854\n",
      "          (excitement, for, research)     0.004854\n",
      "                  (for, research, in)     0.004843\n",
      "           (research, in, artificial)     0.004854\n",
      "             (in, artificial, neural)     0.004843\n",
      "          (neural, networks, causing)     0.004739\n",
      "             (networks, causing, the)     0.004854\n",
      "                   (causing, the, us)     0.004854\n",
      "                (the, us, government)     0.004854\n",
      "                 (us, government, to)     0.004854\n",
      "        (government, to, drastically)     0.004854\n",
      "          (to, drastically, increase)     0.004854\n",
      "     (drastically, increase, funding)     0.004854\n",
      "            (increase, funding, this)     0.004854\n",
      "         (funding, this, contributed)     0.004854\n",
      "              (this, contributed, to)     0.004854\n",
      "               (contributed, to, the)     0.004854\n",
      "                    (to, the, golden)     0.004819\n",
      "                   (the, golden, age)     0.004854\n",
      "                    (golden, age, of)     0.004854\n",
      "                        (age, of, ai)     0.004854\n",
      "                     (of, ai, fueled)     0.004854\n",
      "                     (ai, fueled, by)     0.004854\n",
      "                    (fueled, by, the)     0.004854\n",
      "                (by, the, optimistic)     0.004831\n",
      "            (the, optimistic, claims)     0.004854\n",
      "           (optimistic, claims, made)     0.004854\n",
      "                   (claims, made, by)     0.004854\n",
      "                 (made, by, computer)     0.004854\n",
      "           (by, computer, scientists)     0.004854\n",
      "    (computer, scientists, regarding)     0.004854\n",
      "         (scientists, regarding, the)     0.004854\n",
      "            (regarding, the, ability)     0.004854\n",
      "                   (the, ability, of)     0.004854\n",
      "           (ability, of, perceptrons)     0.004854\n",
      "                (of, perceptrons, to)     0.004854\n",
      "           (perceptrons, to, emulate)     0.004854\n",
      "                 (to, emulate, human)     0.004854\n",
      "       (emulate, human, intelligence)     0.004854\n",
      "           (human, intelligence, the)     0.004854\n",
      "           (intelligence, the, first)     0.004854\n",
      "            (the, first, perceptrons)     0.004796\n",
      "            (first, perceptrons, did)     0.004854\n",
      "              (perceptrons, did, not)     0.004854\n",
      "                     (did, not, have)     0.004843\n",
      "                (not, have, adaptive)     0.004854\n",
      "             (have, adaptive, hidden)     0.004854\n",
      "            (adaptive, hidden, units)     0.004843\n",
      "             (hidden, units, however)     0.004831\n",
      "             (units, however, joseph)     0.004854\n",
      "              (however, joseph, also)     0.004854\n",
      "            (joseph, also, discussed)     0.004854\n",
      "        (also, discussed, multilayer)     0.004854\n",
      " (discussed, multilayer, perceptrons)     0.004854\n",
      "      (multilayer, perceptrons, with)     0.004854\n",
      "              (perceptrons, with, an)     0.004854\n",
      "                 (with, an, adaptive)     0.004854\n",
      "               (an, adaptive, hidden)     0.004854\n",
      "            (adaptive, hidden, layer)     0.004843\n",
      "          (hidden, layer, rosenblatt)     0.004854\n",
      "         (layer, rosenblatt, section)     0.004854\n",
      "         (rosenblatt, section, cited)     0.004854\n",
      "                (section, cited, and)     0.004854\n",
      "                (cited, and, adopted)     0.004854\n",
      "                (and, adopted, these)     0.004854\n",
      "              (adopted, these, ideas)     0.004854\n",
      "                 (these, ideas, also)     0.004854\n",
      "             (ideas, also, crediting)     0.004854\n",
      "              (also, crediting, work)     0.004854\n",
      "                (crediting, work, by)     0.004854\n",
      "                        (work, by, h)     0.004854\n",
      "                           (by, h, d)     0.004854\n",
      "                        (h, d, block)     0.004854\n",
      "                      (d, block, and)     0.004854\n",
      "                      (block, and, b)     0.004854\n",
      "                          (and, b, w)     0.004854\n",
      "                       (b, w, knight)     0.004854\n",
      "           (w, knight, unfortunately)     0.004854\n",
      "       (knight, unfortunately, these)     0.004854\n",
      "        (unfortunately, these, early)     0.004854\n",
      "              (these, early, efforts)     0.004854\n",
      "                (early, efforts, did)     0.004854\n",
      "                  (efforts, did, not)     0.004854\n",
      "                     (did, not, lead)     0.004843\n",
      "                      (not, lead, to)     0.004854\n",
      "                        (lead, to, a)     0.004854\n",
      "                     (to, a, working)     0.004843\n",
      "               (a, working, learning)     0.004854\n",
      "       (working, learning, algorithm)     0.004854\n",
      "           (learning, algorithm, for)     0.004843\n",
      "             (algorithm, for, hidden)     0.004854\n",
      "                 (for, hidden, units)     0.004854\n",
      "                  (hidden, units, ie)     0.004831\n",
      "                    (units, ie, deep)     0.004854\n",
      "                 (ie, deep, learning)     0.004854\n",
      "               (deep, learning, deep)     0.004819\n",
      "           (learning, deep, learning)     0.004854\n",
      "      (deep, learning, breakthroughs)     0.004819\n",
      "        (learning, breakthroughs, in)     0.004854\n",
      "             (breakthroughs, in, the)     0.004854\n",
      "                         (in, the, s)     0.007177\n",
      "                        (the, s, and)     0.007264\n",
      "                          (s, and, s)     0.007264\n",
      "                (and, s, fundamental)     0.004843\n",
      "           (s, fundamental, research)     0.004854\n",
      "         (fundamental, research, was)     0.004854\n",
      "           (research, was, conducted)     0.004854\n",
      "                 (was, conducted, on)     0.004854\n",
      "                (conducted, on, anns)     0.004854\n",
      "                       (on, anns, in)     0.004854\n",
      "                      (anns, in, the)     0.004854\n",
      "                        (and, s, the)     0.004843\n",
      "                      (s, the, first)     0.004854\n",
      "                (the, first, working)     0.004796\n",
      "               (first, working, deep)     0.004854\n",
      "            (working, deep, learning)     0.004854\n",
      "          (deep, learning, algorithm)     0.004819\n",
      "           (learning, algorithm, was)     0.004843\n",
      "                (algorithm, was, the)     0.004854\n",
      "                    (was, the, group)     0.004854\n",
      "                 (the, group, method)     0.004854\n",
      "                  (group, method, of)     0.004854\n",
      "                   (method, of, data)     0.004843\n",
      "                 (of, data, handling)     0.004854\n",
      "                  (data, handling, a)     0.004854\n",
      "                (handling, a, method)     0.004854\n",
      "                      (a, method, to)     0.004854\n",
      "                  (method, to, train)     0.004854\n",
      "             (to, train, arbitrarily)     0.004854\n",
      "           (train, arbitrarily, deep)     0.004854\n",
      "          (arbitrarily, deep, neural)     0.004854\n",
      "        (neural, networks, published)     0.004739\n",
      "            (networks, published, by)     0.004854\n",
      "              (published, by, alexey)     0.004854\n",
      "             (by, alexey, ivakhnenko)     0.004854\n",
      "            (alexey, ivakhnenko, and)     0.004854\n",
      "              (ivakhnenko, and, lapa)     0.004854\n",
      "                      (and, lapa, in)     0.004854\n",
      "                      (lapa, in, the)     0.004854\n",
      "                    (in, the, soviet)     0.004785\n",
      "                 (the, soviet, union)     0.004854\n",
      "                (soviet, union, they)     0.004854\n",
      "              (union, they, regarded)     0.004854\n",
      "                 (they, regarded, it)     0.004854\n",
      "                   (regarded, it, as)     0.004854\n",
      "                          (it, as, a)     0.004854\n",
      "                        (as, a, form)     0.004843\n",
      "                        (a, form, of)     0.004854\n",
      "               (form, of, polynomial)     0.004854\n",
      "         (of, polynomial, regression)     0.004854\n",
      "         (polynomial, regression, or)     0.004854\n",
      "                  (regression, or, a)     0.004854\n",
      "              (or, a, generalization)     0.004854\n",
      "              (a, generalization, of)     0.004854\n",
      "    (generalization, of, rosenblatts)     0.004854\n",
      "        (of, rosenblatts, perceptron)     0.004854\n",
      "         (rosenblatts, perceptron, a)     0.004843\n",
      "               (perceptron, a, paper)     0.004854\n",
      "                (a, paper, described)     0.004854\n",
      "                (paper, described, a)     0.004854\n",
      "                 (described, a, deep)     0.004854\n",
      "                   (a, deep, network)     0.004843\n",
      "                (deep, network, with)     0.004854\n",
      "               (network, with, eight)     0.004854\n",
      "                (with, eight, layers)     0.004854\n",
      "             (eight, layers, trained)     0.004854\n",
      "                (layers, trained, by)     0.004854\n",
      "                  (trained, by, this)     0.004843\n",
      "                   (by, this, method)     0.004854\n",
      "                (this, method, which)     0.004831\n",
      "                  (method, which, is)     0.004854\n",
      "                   (which, is, based)     0.004854\n",
      "                   (based, on, layer)     0.004819\n",
      "                      (on, layer, by)     0.004854\n",
      "                   (layer, by, layer)     0.004854\n",
      "                (by, layer, training)     0.004854\n",
      "           (layer, training, through)     0.004854\n",
      "      (training, through, regression)     0.004854\n",
      "      (through, regression, analysis)     0.004854\n",
      "  (regression, analysis, superfluous)     0.004854\n",
      "      (analysis, superfluous, hidden)     0.004854\n",
      "         (superfluous, hidden, units)     0.004854\n",
      "                 (hidden, units, are)     0.004831\n",
      "                 (units, are, pruned)     0.004854\n",
      "                 (are, pruned, using)     0.004854\n",
      "                   (pruned, using, a)     0.004854\n",
      "                 (using, a, separate)     0.004854\n",
      "            (a, separate, validation)     0.004854\n",
      "          (separate, validation, set)     0.004854\n",
      "             (validation, set, since)     0.004854\n",
      "                    (set, since, the)     0.004854\n",
      "             (since, the, activation)     0.004854\n",
      "         (the, activation, functions)     0.004843\n",
      "          (activation, functions, of)     0.004843\n",
      "                 (functions, of, the)     0.004854\n",
      "                     (of, the, nodes)     0.004785\n",
      "                    (the, nodes, are)     0.004854\n",
      "        (nodes, are, kolmogorovgabor)     0.004854\n",
      "  (are, kolmogorovgabor, polynomials)     0.004854\n",
      "(kolmogorovgabor, polynomials, these)     0.004854\n",
      "           (polynomials, these, were)     0.004854\n",
      "                  (these, were, also)     0.004854\n",
      "                    (were, also, the)     0.004854\n",
      "                   (also, the, first)     0.004854\n",
      "                   (the, first, deep)     0.007194\n",
      "              (first, deep, networks)     0.004843\n",
      "               (deep, networks, with)     0.004854\n",
      "     (networks, with, multiplicative)     0.004854\n",
      "        (with, multiplicative, units)     0.004854\n",
      "          (multiplicative, units, or)     0.004854\n",
      "                   (units, or, gates)     0.004843\n",
      "                     (or, gates, the)     0.004854\n",
      "                  (gates, the, first)     0.004854\n",
      "              (first, deep, learning)     0.004843\n",
      "         (deep, learning, multilayer)     0.004819\n",
      "   (learning, multilayer, perceptron)     0.004854\n",
      "    (multilayer, perceptron, trained)     0.004854\n",
      "            (perceptron, trained, by)     0.004854\n",
      "            (trained, by, stochastic)     0.004843\n",
      "           (by, stochastic, gradient)     0.004854\n",
      "      (stochastic, gradient, descent)     0.007264\n",
      "             (gradient, descent, was)     0.004843\n",
      "            (descent, was, published)     0.004854\n",
      "                 (was, published, in)     0.004854\n",
      "                  (published, in, by)     0.004854\n",
      "                   (in, by, shunichi)     0.004854\n",
      "                (by, shunichi, amari)     0.004854\n",
      "                (shunichi, amari, in)     0.004854\n",
      "                (amari, in, computer)     0.004854\n",
      "          (in, computer, experiments)     0.004854\n",
      "   (computer, experiments, conducted)     0.004854\n",
      "         (experiments, conducted, by)     0.004854\n",
      "              (conducted, by, amaris)     0.004854\n",
      "                (by, amaris, student)     0.004854\n",
      "             (amaris, student, saito)     0.004854\n",
      "                  (student, saito, a)     0.004854\n",
      "                     (saito, a, five)     0.004854\n",
      "                     (a, five, layer)     0.004854\n",
      "                   (five, layer, mlp)     0.004854\n",
      "                   (layer, mlp, with)     0.004854\n",
      "                     (mlp, with, two)     0.004854\n",
      "              (with, two, modifiable)     0.004854\n",
      "            (two, modifiable, layers)     0.004854\n",
      "        (modifiable, layers, learned)     0.004854\n",
      "          (layers, learned, internal)     0.004854\n",
      " (learned, internal, representations)     0.004854\n",
      "      (internal, representations, to)     0.004854\n",
      "      (representations, to, classify)     0.004854\n",
      "         (to, classify, nonlinearily)     0.004854\n",
      "  (classify, nonlinearily, separable)     0.004854\n",
      "   (nonlinearily, separable, pattern)     0.004854\n",
      "        (separable, pattern, classes)     0.004854\n",
      "       (pattern, classes, subsequent)     0.004854\n",
      "  (classes, subsequent, developments)     0.004854\n",
      "       (subsequent, developments, in)     0.004854\n",
      "         (developments, in, hardware)     0.004854\n",
      "                  (in, hardware, and)     0.004854\n",
      "      (hardware, and, hyperparameter)     0.004854\n",
      "       (and, hyperparameter, tunings)     0.004854\n",
      "      (hyperparameter, tunings, have)     0.004854\n",
      "                (tunings, have, made)     0.004854\n",
      "               (have, made, endtoend)     0.004854\n",
      "         (made, endtoend, stochastic)     0.004854\n",
      "     (endtoend, stochastic, gradient)     0.004854\n",
      "             (gradient, descent, the)     0.004843\n",
      "            (descent, the, currently)     0.004854\n",
      "           (the, currently, dominant)     0.004854\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Compute Probabilities with Smoothing\n",
    "bigram_probabilities = {bi_: (count + 1) / (uni_grams[bi_[0]] + vocab) for bi_, count in bi_grams.items()}\n",
    "trigram_probabilities = {tri_: (count + 1) / (bi_grams.get((tri_[0], tri_[1]), 0) + vocab) for tri_, count in tri_grams.items()}\n",
    "\n",
    "# Convert dictionaries to DataFrames\n",
    "bigram_df = pd.DataFrame(bigram_probabilities.items(), columns=['Bigram**', '**Probability'])\n",
    "trigram_df = pd.DataFrame(trigram_probabilities.items(), columns=['Trigram', 'Probability'])\n",
    "\n",
    "# Display tables\n",
    "print(\"Bigram Probabilities:\")\n",
    "print(bigram_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nTrigram Probabilities:\")\n",
    "print(trigram_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity Computation for Bigram and Trigram Models\n",
    "Perplexity is a **measure of how well a probabilistic language model predicts a test sample**. A *lower perplexity score indicates a better model*.\n",
    "\n",
    "The formula for perplexity $( PPL )$ of a sentence given a language model is:\n",
    "\n",
    "$$\n",
    "PPL = e^{- \\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i | context)}\n",
    "$$\n",
    "\n",
    "`where:`\n",
    "> $ ( N ) $ is the total number of words in the test sentence.\\\n",
    "> $ ( P(w_i | context) ) $ is the probability of word $( w_i )$ given its previous words.\\\n",
    "> $( context ) $ depends on whether we are using a **bigram** or **trigram** model.\n",
    "\n",
    "\n",
    "### Computing Log Probabilities\n",
    "We compute the log probability sum using the given probability model.\n",
    "\n",
    "#### **For Bigram Model:**\n",
    "A bigram model estimates the probability of a word given the previous word:\n",
    "$$\n",
    "P(w_i | w_{i-1}) = \\frac{Count(w_{i-1}, w_i)}{Count(w_{i-1})}\n",
    "$$\n",
    "\n",
    "\n",
    "> If a bigram is not found in the model, we apply **Laplace smoothing** by adding 1 to the denominator.\n",
    "\n",
    "#### **For Trigram Model:**\n",
    "A trigram model estimates the probability of a word given the previous two words:\n",
    "$$\n",
    "P(w_i | w_{i-2}, w_{i-1}) = \\frac{Count(w_{i-2}, w_{i-1}, w_i)}{Count(w_{i-2}, w_{i-1})}\n",
    "$$\n",
    "\n",
    "The log probability sum is computed as:\n",
    "```python\n",
    "    log_prob_sum = sum(\n",
    "        np.log(trigram_probabilities.get(tg, 1 / (bi_grams.get((tg[0], tg[1]), 0) + vocab)))\n",
    "        for tg in trigrams(test_tokens)\n",
    "    )\n",
    "```\n",
    "> Here, **bi-grams** are used in the denominator to normalize the probability.\n",
    "\n",
    "#### 3. Computing Perplexity\n",
    "Once the log probability sum is computed, we calculate perplexity:\n",
    "```python\n",
    "    return np.exp(-log_prob_sum / N)\n",
    "```\n",
    "This applies the **exponential function** to the negative average log probability to get the final perplexity score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_perplexity(test_sentence, model):\n",
    "    \"\"\"Compute the perplexity of a sentence based on a bigram or trigram model.\"\"\"\n",
    "    \n",
    "    test_tokens = [word.lower() for word in word_tokenize(test_sentence) if word.isalnum()]\n",
    "    N = len(test_tokens)\n",
    "    \n",
    "    if N == 0:\n",
    "        return float('inf')  # Handle edge case of empty input\n",
    "\n",
    "    log_prob_sum = 0\n",
    "\n",
    "    if model == \"bigram\":\n",
    "        log_prob_sum = sum(\n",
    "            np.log(bigram_probabilities.get(bg, 1 / (uni_grams.get(bg[0], 0) + vocab)))\n",
    "            for bg in bigrams(test_tokens)\n",
    "        )\n",
    "\n",
    "    elif model == \"trigram\":\n",
    "        log_prob_sum = sum(\n",
    "            np.log(trigram_probabilities.get(tg, 1 / (bi_grams.get((tg[0], tg[1]), 0) + vocab)))\n",
    "            for tg in trigrams(test_tokens)\n",
    "        )\n",
    "\n",
    "    return np.exp(-log_prob_sum / N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity Scores for Test Sentence: \"Resnet18 is a type of neural network architecture that deals with the vanishing gradient problem.\"\n",
      "\n",
      "🔹 Bigram Model Perplexity Score : 210.7039\n",
      "🔹 Trigram Model Perplexity Score : 184.5442\n"
     ]
    }
   ],
   "source": [
    "# Example Test Sentence\n",
    "test_sentence = \"Resnet18 is a type of neural network architecture that deals with the vanishing gradient problem.\"\n",
    "\n",
    "# Compute Perplexity Scores\n",
    "bigram_score = compute_perplexity(test_sentence, \"bigram\")\n",
    "trigram_score = compute_perplexity(test_sentence, \"trigram\")\n",
    "\n",
    "# Print results with better formatting\n",
    "print(f\"\\nPerplexity Scores for Test Sentence: \\\"{test_sentence}\\\"\\n\")\n",
    "print(f\"🔹 Bigram Model Perplexity Score : {bigram_score:.4f}\")\n",
    "print(f\"🔹 Trigram Model Perplexity Score : {trigram_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
